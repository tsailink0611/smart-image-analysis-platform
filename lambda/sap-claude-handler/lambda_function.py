# lambda_function.py
# Stable, no external deps. Reads salesData (array) or csv (string). Bedrock converse. CORS/OPTIONS ready.

import json, os, base64, logging, boto3
from collections import Counter, defaultdict
from typing import Any, Dict, List, Optional, Tuple

# ====== ENV ======
MODEL_ID       = os.environ.get("BEDROCK_MODEL_ID", "us.deepseek.r1-v1:0")
REGION         = os.environ.get("AWS_REGION", os.environ.get("AWS_DEFAULT_REGION", "us-east-1"))
DEFAULT_FORMAT = (os.environ.get("DEFAULT_FORMAT", "json") or "json").lower()  # 'json'|'markdown'|'text'
MAX_TOKENS     = int(os.environ.get("MAX_TOKENS", "2200"))
TEMPERATURE    = float(os.environ.get("TEMPERATURE", "0.15"))

# ====== LOG ======
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ====== CORS/Response ======
def response_json(status: int, body: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "statusCode": status,
        "headers": {
            "Content-Type": "application/json; charset=utf-8",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Content-Type, Authorization, X-Requested-With",
            "Access-Control-Allow-Methods": "OPTIONS,POST"
        },
        "body": json.dumps(body, ensure_ascii=False)
    }

# ====== Debug early echo (enable with LAMBDA_DEBUG_ECHO=1 or ?echo=1) ======
def _early_echo(event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    try:
        qs = (event.get("rawQueryString") or "").lower()
        env_on = os.environ.get("LAMBDA_DEBUG_ECHO") in ("1", "true", "TRUE")
        if not (env_on or ("echo=1" in qs)):
            return None
        body_raw = event.get("body")
        if event.get("isBase64Encoded") and isinstance(body_raw, str):
            try:
                body_raw = base64.b64decode(body_raw).decode("utf-8-sig")
            except Exception:
                body_raw = "<base64 decode error>"
        elif isinstance(body_raw, (bytes, bytearray)):
            try:
                body_raw = body_raw.decode("utf-8-sig")
            except Exception:
                body_raw = body_raw.decode("utf-8", errors="ignore")
        sample = body_raw[:1000] if isinstance(body_raw, str) else str(type(body_raw))
        return response_json(200, {
            "message": "DEBUG",
            "format": "json",
            "engine": "bedrock",
            "model": MODEL_ID,
            "response": {
                "echo": "early",
                "received_type": type(body_raw).__name__ if body_raw is not None else "None",
                "raw_sample": sample
            }
        })
    except Exception:
        return None

# ====== Helpers ======
def _to_number(x: Any) -> float:
    try:
        s = str(x).replace(",", "").replace("Â¥", "").replace("å††", "").strip()
        return float(s)
    except Exception:
        return 0.0

def _detect_columns(rows: List[Dict[str, Any]]) -> Dict[str, str]:
    colmap: Dict[str, str] = {}
    if not rows:
        return colmap
    for c in rows[0].keys():
        name = str(c)
        lc = name.lower()
        if ("æ—¥" in name) or ("date" in lc):
            colmap.setdefault("date", name)
        if ("å£²" in name) or ("é‡‘é¡" in name) or ("amount" in lc) or ("sales" in lc) or ("total" in lc):
            colmap.setdefault("sales", name)
        if ("å•†" in name) or ("å“" in name) or ("product" in lc) or ("item" in lc) or ("name" in lc):
            colmap.setdefault("product", name)
    return colmap

def _compute_stats(rows: List[Dict[str, Any]]) -> Dict[str, Any]:
    total = len(rows)
    if total == 0:
        return {"total_rows": 0, "total_sales": 0.0, "avg_row_sales": 0.0, "top_products": [], "timeseries": []}

    colmap = _detect_columns(rows)
    dcol, scol, pcol = colmap.get("date"), colmap.get("sales"), colmap.get("product")

    ts = defaultdict(float)
    by_product: Counter = Counter()
    total_sales = 0.0

    for r in rows:
        v = _to_number(r.get(scol, 0)) if scol else 0.0
        total_sales += v
        if pcol:
            by_product[str(r.get(pcol, "")).strip()] += v
        if dcol:
            dt = str(r.get(dcol, "")).strip().replace("/", "-")
            day = dt[:10] if len(dt) >= 10 else dt
            if day:
                ts[day] += v

    top_products = [{"name": k, "sales": float(v)} for k, v in by_product.most_common(5)]
    trend = [{"date": d, "sales": float(v)} for d, v in sorted(ts.items())]
    avg = float(total_sales / total) if total else 0.0

    return {
        "total_rows": total,
        "total_sales": float(total_sales),
        "avg_row_sales": avg,
        "top_products": top_products,
        "timeseries": trend
    }

def _build_prompt_json(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    schema_hint = {
        "type": "object",
        "properties": {
            "overview": {"type": "string"},
            "findings": {"type": "array", "items": {"type": "string"}},
            "kpis": {
                "type": "object",
                "properties": {
                    "total_sales": {"type": "number"},
                    "top_products": {
                        "type": "array",
                        "items": {"type": "object", "properties": {"name": {"type": "string"}, "sales": {"type": "number"}}}
                    }
                }
            },
            "trend": {"type": "array", "items": {"type": "object", "properties": {"date": {"type": "string"}, "sales": {"type": "number"}}}}
        },
        "required": ["overview", "findings", "kpis"]
    }
    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†ææŒ‡ç¤º
    analysis_instructions = _get_analysis_instructions(data_type)
    
    return f"""ã‚ãªãŸã¯ä¼æ¥­ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹çµŒé¨“è±Šå¯ŒãªçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€çµŒå–¶é™£ã«åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã€‘: {_get_data_type_name(data_type)}

ã€åˆ†ææŒ‡ç¤ºã€‘
{analysis_instructions}

ã€å‡ºåŠ›å½¢å¼ã€‘
- è‡ªç„¶ãªæ—¥æœ¬èªã§ã€ã¾ã‚‹ã§åŒåƒšã«èª¬æ˜ã™ã‚‹ã‚ˆã†ã«æ›¸ã„ã¦ãã ã•ã„
- å°‚é–€ç”¨èªã¯å¿…è¦æœ€å°é™ã«ç•™ã‚ã€èª°ã§ã‚‚ç†è§£ã§ãã‚‹è¡¨ç¾ã‚’ä½¿ã£ã¦ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹åƒå††ã€ãªã©ã€æ—¥æœ¬äººãŒæ™®æ®µä½¿ã†è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„
- çµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ãªå½¢ã§æ•´ç†ã—ã¦ãã ã•ã„ï¼š
  - ã€Œå…¨ä½“ã®çŠ¶æ³ã€: ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’2-3è¡Œã§
  - ã€Œæ°—ã¥ã„ãŸã“ã¨ã€: é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’3ã¤ã¾ã§
  - ã€Œæ•°å­—ã®ã¾ã¨ã‚ã€: ä¸»è¦ãªæŒ‡æ¨™

â€»ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ã£ã¦åˆ†æã—ã¦ãã ã•ã„ï¼ˆæ¨æ¸¬ã¯é¿ã‘ã¦ãã ã•ã„ï¼‰
â€»ä»¥ä¸‹ã®å½¢å¼ã§JSONã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„: {json.dumps(schema_hint, ensure_ascii=False)}

[çµ±è¨ˆè¦ç´„]
{json.dumps(stats, ensure_ascii=False)}

[ã‚µãƒ³ãƒ—ãƒ«è¡Œ]
{json.dumps(sample, ensure_ascii=False)}
"""

def _build_prompt_markdown(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    return f"""ã‚ãªãŸã¯ä¼šç¤¾ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€ç¤¾é•·ã‚„éƒ¨é•·ãŒèª­ã‚€ãƒ¬ãƒãƒ¼ãƒˆã‚’ã€å®Œå…¨ã«æ—¥æœ¬èªã¨æ•°å­—ã ã‘ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ã€‘
- Markdownã‚„è¨˜å·ã¯ä¸€åˆ‡ä½¿ã‚ãšã€æ™®é€šã®æ—¥æœ¬èªæ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„
- ã€Œ##ã€ã€Œ**ã€ã€Œ|ã€ã€Œ-ã€ãªã©ã®è¨˜å·ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„
- è‹±èªã‚„å°‚é–€ç”¨èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„
- ã¾ã‚‹ã§éƒ¨ä¸‹ãŒä¸Šå¸ã«å£é ­ã§å ±å‘Šã™ã‚‹ã‚ˆã†ãªã€è‡ªç„¶ãªæ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹%å¢—åŠ ã€ãªã©ã€æ—¥æœ¬äººãŒè©±ã™ã¨ãã®è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„

# çµ±è¨ˆè¦ç´„
{json.dumps(stats, ensure_ascii=False)}

# ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ï¼‰
{json.dumps(sample, ensure_ascii=False)}
"""

def _build_prompt_text(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    return f"""ã‚ãªãŸã¯ä¼šç¤¾ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€ä¸Šå¸ã«å£é ­ã§å ±å‘Šã™ã‚‹ã‚ˆã†ã«ã€å®Œå…¨ã«æ—¥æœ¬èªã ã‘ã§3è¡Œä»¥å†…ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾å®ˆã‚‹ã“ã¨ã€‘
- è¨˜å·ã€è‹±èªã€ã‚«ã‚¿ã‚«ãƒŠå°‚é–€ç”¨èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹%å¢—åŠ ã€ãªã©ã€æ™®é€šã«è©±ã™ã¨ãã®è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„
- ã¾ã‚‹ã§æœç¤¼ã§å ±å‘Šã™ã‚‹ã‚ˆã†ãªã€è‡ªç„¶ãªè©±ã—è¨€è‘‰ã§æ›¸ã„ã¦ãã ã•ã„
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§ã€ä¸å¯§ã«æ›¸ã„ã¦ãã ã•ã„

[çµ±è¨ˆè¦ç´„]
{json.dumps(stats, ensure_ascii=False)}

[ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ï¼‰]
{json.dumps(sample, ensure_ascii=False)}
"""

def _parse_csv_simple(csv_text: str) -> List[Dict[str, Any]]:
    lines = [l for l in csv_text.splitlines() if l.strip() != ""]
    if not lines: return []
    headers = [h.strip() for h in lines[0].split(",")]
    rows: List[Dict[str, Any]] = []
    for line in lines[1:]:
        cells = [c.strip() for c in line.split(",")]
        row = {}
        for i, h in enumerate(headers):
            row[h] = cells[i] if i < len(cells) else ""
        rows.append(row)
    return rows

def _identify_data_type(columns: List[str], sample_data: List[Dict[str, Any]]) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã¨ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’è‡ªå‹•åˆ¤åˆ¥ï¼ˆ7ã¤ã®åˆ†æã‚¿ã‚¤ãƒ—ã«ç‰¹åŒ–ï¼‰"""
    if not columns:
        return "financial_data"
    
    # åˆ—åã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã¦åˆ¤åˆ¥ã—ã‚„ã™ãã™ã‚‹
    col_lower = [col.lower() for col in columns]
    col_str = " ".join(col_lower) + " " + " ".join(columns)
    
    # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
    scores = {
        "hr_data": 0,
        "marketing_data": 0,
        "sales_data": 0,
        "financial_data": 0,
        "inventory_data": 0,
        "customer_data": 0
    }
    
    # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
    hr_strong_keywords = ["ç¤¾å“¡id", "employee", "æ°å", "éƒ¨ç½²", "çµ¦ä¸", "salary", "è³ä¸", "å¹´å", "è©•ä¾¡", "performance", "æ®‹æ¥­", "overtime", "æœ‰çµ¦", "é›¢è·", "æ˜‡é€²", "ã‚¹ã‚­ãƒ«", "ãƒãƒ¼ãƒ è²¢çŒ®", "äººäº‹"]
    for keyword in hr_strong_keywords:
        if keyword in col_str:
            scores["hr_data"] += 3
    
    # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    hr_medium_keywords = ["å‹¤æ€ ", "attendance", "ç ”ä¿®", "training", "ç›®æ¨™é”æˆ", "è·ä½", "å…¥ç¤¾", "å¹´é½¢"]
    for keyword in hr_medium_keywords:
        if keyword in col_str:
            scores["hr_data"] += 2
    
    # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    marketing_strong_keywords = ["ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", "campaign", "roi", "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³", "impression", "ã‚¯ãƒªãƒƒã‚¯", "click", "cvæ•°", "conversion", "é¡§å®¢ç²å¾—", "cac", "roas", "åºƒå‘Š", "åª’ä½“", "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ"]
    for keyword in marketing_strong_keywords:
        if keyword in col_str:
            scores["marketing_data"] += 3
    
    # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    marketing_medium_keywords = ["äºˆç®—", "budget", "æ”¯å‡º", "cost", "facebook", "google", "youtube", "instagram", "tiktok", "twitter"]
    for keyword in marketing_medium_keywords:
        if keyword in col_str:
            scores["marketing_data"] += 1
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sales_strong_keywords = ["å£²ä¸Š", "sales", "revenue", "å•†å“", "product", "é¡§å®¢", "customer", "é‡‘é¡", "amount", "å˜ä¾¡", "price", "æ•°é‡", "quantity"]
    for keyword in sales_strong_keywords:
        if keyword in col_str:
            scores["sales_data"] += 3
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sales_medium_keywords = ["æ—¥ä»˜", "date", "åº—èˆ—", "store", "åœ°åŸŸ", "region", "ã‚«ãƒ†ã‚´ãƒª", "category"]
    for keyword in sales_medium_keywords:
        if keyword in col_str:
            scores["sales_data"] += 1
    
    # çµ±åˆæˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿ï¼ˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿ï¼‰ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    financial_strong_keywords = ["å£²ä¸Šé«˜", "revenue", "åˆ©ç›Š", "profit", "è³‡ç”£", "asset", "è² å‚µ", "liability", "ã‚­ãƒ£ãƒƒã‚·ãƒ¥", "cash", "æç›Š", "pl", "è²¸å€Ÿ", "bs"]
    for keyword in financial_strong_keywords:
        if keyword in col_str:
            scores["financial_data"] += 3
    
    # åœ¨åº«åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    inventory_strong_keywords = ["åœ¨åº«", "inventory", "stock", "åœ¨åº«æ•°", "ä¿æœ‰æ•°", "å€‰åº«", "warehouse", "å›è»¢ç‡", "turnover", "æ»ç•™", "å…¥åº«", "å‡ºåº«", "èª¿é”", "procurement"]
    for keyword in inventory_strong_keywords:
        if keyword in col_str:
            scores["inventory_data"] += 3
    
    # åœ¨åº«åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    inventory_medium_keywords = ["å•†å“ã‚³ãƒ¼ãƒ‰", "sku", "ãƒ­ãƒƒãƒˆ", "lot", "å“ç•ª", "å‹ç•ª", "ä»•å…¥", "supplier", "ç™ºæ³¨", "order", "ç´æœŸ", "delivery"]
    for keyword in inventory_medium_keywords:
        if keyword in col_str:
            scores["inventory_data"] += 1
    
    # é¡§å®¢åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
    customer_strong_keywords = ["é¡§å®¢", "customer", "ä¼šå“¡", "member", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "user", "ltv", "lifetime", "churn", "é›¢è„±", "ç¶™ç¶š", "retention", "æº€è¶³åº¦", "satisfaction"]
    for keyword in customer_strong_keywords:
        if keyword in col_str:
            scores["customer_data"] += 3
    
    # é¡§å®¢åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    customer_medium_keywords = ["ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", "segment", "å¹´é½¢", "age", "æ€§åˆ¥", "gender", "åœ°åŸŸ", "region", "è³¼å…¥å±¥æ­´", "purchase", "ã‚¢ã‚¯ã‚»ã‚¹", "access", "ã‚¯ãƒªãƒƒã‚¯", "click"]
    for keyword in customer_medium_keywords:
        if keyword in col_str:
            scores["customer_data"] += 1
    
    # ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‹ã‚‰ã‚‚åˆ¤å®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    if sample_data and len(sample_data) > 0:
        sample = sample_data[0]
        
        # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´çš„ãªå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
        for key, value in sample.items():
            str_value = str(value).lower()
            
            # äººäº‹ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(dept in str_value for dept in ["å–¶æ¥­éƒ¨", "itéƒ¨", "äººäº‹éƒ¨", "è²¡å‹™éƒ¨", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨"]):
                scores["hr_data"] += 5
            if any(pos in str_value for pos in ["ä¸»ä»»", "ä¿‚é•·", "ä¸€èˆ¬", "éƒ¨é•·", "èª²é•·"]):
                scores["hr_data"] += 3
            if any(risk in str_value for risk in ["ä½", "ä¸­", "é«˜"]) and ("ãƒªã‚¹ã‚¯" in key or "risk" in key.lower()):
                scores["hr_data"] += 4
                
            # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(media in str_value for media in ["googleåºƒå‘Š", "facebookåºƒå‘Š", "youtubeåºƒå‘Š", "instagramåºƒå‘Š", "lineåºƒå‘Š", "tiktokåºƒå‘Š"]):
                scores["marketing_data"] += 5
            if "%" in str_value and any(metric in key.lower() for metric in ["roi", "é”æˆç‡", "æº€è¶³åº¦"]):
                scores["marketing_data"] += 2
                
            # å£²ä¸Šç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•°å€¤ãŒå¤§ããã€å•†å“åãŒã‚ã‚‹å ´åˆï¼‰
            if "å•†å“" in key or "product" in key.lower():
                scores["sales_data"] += 3
            if key.lower() in ["åº—èˆ—", "store"] and str_value:
                scores["sales_data"] += 4
                
            # åœ¨åº«ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(unit in str_value for unit in ["å€‹", "æœ¬", "kg", "ç®±", "ã‚»ãƒƒãƒˆ", "å°"]):
                scores["inventory_data"] += 2
            if "warehouse" in key.lower() or "å€‰åº«" in key:
                scores["inventory_data"] += 3
            if any(status in str_value for status in ["å…¥è·å¾…ã¡", "å‡ºè·æ¸ˆã¿", "åœ¨åº«åˆ‡ã‚Œ", "èª¿é”ä¸­"]):
                scores["inventory_data"] += 4
                
            # é¡§å®¢ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³  
            if any(age in str_value for age in ["20ä»£", "30ä»£", "40ä»£", "50ä»£", "60ä»£"]) or str_value.isdigit() and 18 <= int(str_value) <= 80:
                scores["customer_data"] += 3
            if any(gender in str_value for gender in ["ç”·æ€§", "å¥³æ€§", "male", "female", "ç”·", "å¥³"]):
                scores["customer_data"] += 3
            if "@" in str_value:  # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
                scores["customer_data"] += 4
    
    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™
    if max(scores.values()) > 0:
        return max(scores, key=scores.get)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return "financial_data"

def _get_data_type_name(data_type: str) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ—¥æœ¬èªåã‚’è¿”ã™"""
    type_names = {
        "pl_statement": "æç›Šè¨ˆç®—æ›¸ï¼ˆPLè¡¨ï¼‰",
        "balance_sheet": "è²¸å€Ÿå¯¾ç…§è¡¨ï¼ˆBSï¼‰",
        "cashflow_statement": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸",
        "sales_data": "å£²ä¸Šãƒ‡ãƒ¼ã‚¿",
        "inventory_data": "åœ¨åº«ãƒ‡ãƒ¼ã‚¿",
        "customer_data": "é¡§å®¢ãƒ‡ãƒ¼ã‚¿",
        "hr_data": "äººäº‹ãƒ‡ãƒ¼ã‚¿",
        "marketing_data": "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿",
        "financial_data": "è²¡å‹™ãƒ‡ãƒ¼ã‚¿",
        "document_data": "æ›¸é¡ç”»åƒãƒ‡ãƒ¼ã‚¿",
        "unknown": "ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿"
    }
    return type_names.get(data_type, "è²¡å‹™ãƒ‡ãƒ¼ã‚¿")

def validate_analysis_compatibility(detected_data_type: str, requested_analysis_type: str) -> Tuple[bool, str]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨åˆ†æã‚¿ã‚¤ãƒ—ã®é©åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä½¿ã„ã‚„ã™ã•é‡è¦–ï¼‰"""
    # é©åˆæ€§ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ - ã‚ˆã‚ŠæŸ”è»Ÿã«
    compatibility_matrix = {
        'sales': {
            'primary': ['sales_data'],  # ä¸»è¦å¯¾å¿œ
            'secondary': ['financial_data'],  # å‰¯æ¬¡å¯¾å¿œï¼ˆè­¦å‘Šãªã—ã§é€šã™ï¼‰
            'name': 'å£²ä¸Šåˆ†æ',
            'description': 'å£²ä¸Šãƒ»å•†å“ãƒ»é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'hr': {
            'primary': ['hr_data'],
            'secondary': [],  # äººäº‹ã¯å³å¯†ã«
            'name': 'äººäº‹åˆ†æ', 
            'description': 'å¾“æ¥­å“¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»çµ¦ä¸ãƒ»è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'marketing': {
            'primary': ['marketing_data'],
            'secondary': ['financial_data'],  # äºˆç®—ãƒ‡ãƒ¼ã‚¿ãªã©ã‚‚å¯
            'name': 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æ',
            'description': 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ»ROIãƒ»é¡§å®¢ç²å¾—ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'strategic': {
            'primary': ['financial_data', 'sales_data'],
            'secondary': ['hr_data', 'marketing_data'],  # çµ±åˆæˆ¦ç•¥ã¯ä½•ã§ã‚‚å¯
            'name': 'çµ±åˆæˆ¦ç•¥åˆ†æ',
            'description': 'ç·åˆçš„ãªãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã®æˆ¦ç•¥åˆ†æ'
        }
    }
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯é€šã™
    if requested_analysis_type not in compatibility_matrix:
        return True, ""
    
    config = compatibility_matrix[requested_analysis_type]
    
    # ä¸»è¦ã‚¿ã‚¤ãƒ—ã¾ãŸã¯å‰¯æ¬¡ã‚¿ã‚¤ãƒ—ã«é©åˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    all_allowed = config['primary'] + config['secondary']
    
    if detected_data_type in all_allowed:
        return True, ""  # é©åˆã—ã¦ã„ã‚‹
    
    # ä¸é©åˆã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
    if detected_data_type not in all_allowed:
        # æœ€é©ãªãƒœã‚¿ãƒ³ã‚’ææ¡ˆ
        best_match = None
        for btn_type, btn_config in compatibility_matrix.items():
            if detected_data_type in (btn_config['primary'] + btn_config['secondary']):
                best_match = btn_config['name']
                break
        
        error_msg = f"""âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ä¸ä¸€è‡´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ

ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {_get_data_type_name(detected_data_type)}
é¸æŠã•ã‚ŒãŸåˆ†æ: {config['name']}

ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯{config['name']}ã«ã¯æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"""
        
        if best_match:
            error_msg += f"\n\nğŸ’¡ ã“ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€Œ{best_match}ã€ãŒãŠã™ã™ã‚ã§ã™ã€‚\n\nãŸã ã—ã€ãã®ã¾ã¾åˆ†æã‚’ç¶šè¡Œã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚"
            # è­¦å‘Šã ã‘ã§ç¶šè¡Œã‚’è¨±å¯
            return True, ""
        else:
            error_msg += f"\n\nã€Œçµ±åˆæˆ¦ç•¥åˆ†æã€ãƒœã‚¿ãƒ³ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
            return True, ""
    
    return True, ""

def _get_analysis_instructions(data_type: str) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†ææŒ‡ç¤ºã‚’è¿”ã™"""
    instructions = {
        "pl_statement": """
- å£²ä¸Šé«˜ã€å£²ä¸ŠåŸä¾¡ã€ç²—åˆ©ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- è²©ç®¡è²»ã®å†…è¨³ã¨å£²ä¸Šé«˜ã«å ã‚ã‚‹å‰²åˆã‚’åˆ†æã—ã¦ãã ã•ã„
- å–¶æ¥­åˆ©ç›Šã€çµŒå¸¸åˆ©ç›Šã€å½“æœŸç´”åˆ©ç›Šã®æ¨ç§»ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- åç›Šæ€§ã®å¥å…¨æ€§ã¨æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„""",
        
        "balance_sheet": """
- ç·è³‡ç”£ã€æµå‹•è³‡ç”£ã€å›ºå®šè³‡ç”£ã®æ§‹æˆã‚’ç¢ºèªã—ã¦ãã ã•ã„
- è² å‚µã¨ç´”è³‡ç”£ã®ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ†æã—ã¦ãã ã•ã„
- æµå‹•æ¯”ç‡ã€è‡ªå·±è³‡æœ¬æ¯”ç‡ãªã©ã®å®‰å…¨æ€§æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„
- è²¡å‹™ã®å¥å…¨æ€§ã¨è³‡é‡‘ç¹°ã‚Šã«ã¤ã„ã¦è©•ä¾¡ã—ã¦ãã ã•ã„""",
        
        "cashflow_statement": """
- å–¶æ¥­ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã€æŠ•è³‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã€è²¡å‹™ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- ç¾é‡‘å‰µå‡ºèƒ½åŠ›ã¨è³‡é‡‘ã®ä½¿ã„é“ã‚’åˆ†æã—ã¦ãã ã•ã„
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã®å¥å…¨æ€§ã¨æŒç¶šå¯èƒ½æ€§ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- è³‡é‡‘ç¹°ã‚Šã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„""",
        
        "sales_data": """
ã€å£²ä¸Šæˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®åç›Šåˆ†æã‚’å®Ÿè¡Œã€‘

**1. å¤šæ¬¡å…ƒå£²ä¸Šåˆ†æãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰è¨ºæ–­**
- å£²ä¸Šé«˜ãƒ»ç²—åˆ©ãƒ»ç´”åˆ©ã®è©³ç´°æ§‹é€ åˆ†æï¼ˆå‰å¹´åŒæœŸæ¯”ãƒ»æˆé•·ç‡ãƒ»åç›Šæ€§è©•ä¾¡ï¼‰
- å•†å“åˆ¥ãƒ»é¡§å®¢åˆ¥ãƒ»ãƒãƒ£ãƒãƒ«åˆ¥ãƒ»åœ°åŸŸåˆ¥ã®å£²ä¸Šè²¢çŒ®åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨é›†ä¸­åº¦åˆ†æ
- å­£ç¯€æ€§ãƒ»å¾ªç’°æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰æˆåˆ†ã®åˆ†è§£ã¨å°†æ¥6ãƒ¶æœˆé–“ã®å£²ä¸Šäºˆæ¸¬
- ä¾¡æ ¼å¼¾åŠ›æ€§åˆ†æã«ã‚ˆã‚‹æœ€é©ä¾¡æ ¼æˆ¦ç•¥ã¨åç›Šæœ€å¤§åŒ–ãƒã‚¤ãƒ³ãƒˆã®ç‰¹å®š

**2. åç›Šæ§‹é€ ãƒ»åˆ©ç›Šæ€§ã®åŒ…æ‹¬åˆ†æ**
- å£²ä¸Šé«˜ç·åˆ©ç›Šç‡ãƒ»å–¶æ¥­åˆ©ç›Šç‡ãƒ»ROSã®çµŒå¹´å¤‰åŒ–ã¨æ¥­ç•Œæ¨™æº–æ¯”è¼ƒ
- å•†å“ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆã‚¹ã‚¿ãƒ¼ãƒ»å•é¡Œå…ãƒ»é‡‘ã®ãªã‚‹æœ¨ãƒ»è² ã‘çŠ¬ã®åˆ†é¡ï¼‰
- é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã®åç›Šæ€§åˆ†æï¼ˆãƒ‘ãƒ¬ãƒ¼ãƒˆæ³•å‰‡80/20ã®æ¤œè¨¼ï¼‰
- å›ºå®šè²»ãƒ»å¤‰å‹•è²»æ§‹é€ ã¨æç›Šåˆ†å²ç‚¹åˆ†æã«ã‚ˆã‚‹çµŒå–¶å®‰å…¨æ€§è©•ä¾¡

**3. å¸‚å ´æ©Ÿä¼šãƒ»æˆé•·æˆ¦ç•¥ã®ç™ºè¦‹**
- æˆé•·ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ç‰¹å®šï¼ˆå•†å“ãƒ»å¸‚å ´ãƒ»é¡§å®¢ãƒ»ãƒãƒ£ãƒãƒ«åˆ¥æˆé•·è¦å› åˆ†æï¼‰
- æœªé–‹æ‹“å¸‚å ´ãƒ»ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ãƒ»ã‚¢ãƒƒãƒ—ã‚»ãƒ«æ©Ÿä¼šã®å®šé‡çš„è©•ä¾¡
- ç«¶åˆã‚·ã‚§ã‚¢åˆ†æã¨å¸‚å ´ãƒã‚¸ã‚·ãƒ§ãƒ³å¼·åŒ–ã®ãŸã‚ã®æˆ¦ç•¥çš„æŠ•è³‡ææ¡ˆ
- æ–°è¦äº‹æ¥­ãƒ»å•†å“ãƒ©ã‚¤ãƒ³æ‹¡å¼µã®åç›Šæ€§äºˆæ¸¬ã¨ãƒªã‚¹ã‚¯è©•ä¾¡

**4. å–¶æ¥­åŠ¹ç‡ãƒ»ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–**
- å–¶æ¥­æ‹…å½“è€…åˆ¥ãƒ»ãƒãƒ¼ãƒ åˆ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨æ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ç®—å‡º
- ã‚»ãƒ¼ãƒ«ã‚¹ãƒ•ã‚¡ãƒãƒ«å„æ®µéšã®è»¢æ›ç‡åˆ†æã¨æ­©ç•™ã¾ã‚Šæ”¹å–„ç­–
- å•†è«‡æˆç´„ç‡ãƒ»å¹³å‡å—æ³¨é¡ãƒ»ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ åˆ†æã«ã‚ˆã‚‹å–¶æ¥­ãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–
- å–¶æ¥­æŠ•è³‡ROIã¨äººå“¡é…ç½®æœ€é©åŒ–ã«ã‚ˆã‚‹å£²ä¸Šå‘ä¸Šã‚·ãƒŠãƒªã‚ª

**5. æˆ¦ç•¥å®Ÿè¡Œãƒ»KPIè¨­å®šãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**
- çŸ­æœŸï¼ˆå››åŠæœŸï¼‰ãƒ»ä¸­æœŸï¼ˆå¹´æ¬¡ï¼‰ãƒ»é•·æœŸï¼ˆ3å¹´ï¼‰ã®å£²ä¸Šç›®æ¨™ã¨æˆ¦ç•¥ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
- é‡ç‚¹ç®¡ç†ã™ã¹ãå…ˆè¡ŒæŒ‡æ¨™ãƒ»é…è¡ŒæŒ‡æ¨™ã®ç‰¹å®šã¨ç›®æ¨™å€¤è¨­å®š
- ç«¶åˆå¯¾ç­–ãƒ»å¸‚å ´å¤‰åŒ–å¯¾å¿œã®ãŸã‚ã®æ©Ÿå‹•çš„æˆ¦ç•¥ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æº–å‚™
- å£²ä¸Šæˆé•·ã‚’æ”¯ãˆã‚‹çµ„ç¹”ãƒ»ã‚·ã‚¹ãƒ†ãƒ æŠ•è³‡è¨ˆç”»ã¨æœŸå¾…ROIç®—å‡º""",
        
        "inventory_data": """
- åœ¨åº«ã®ç·é¡ã€å•†å“åˆ¥æ§‹æˆã‚’ç¢ºèªã—ã¦ãã ã•ã„
- åœ¨åº«å›è»¢ç‡ã‚„æ»ç•™åœ¨åº«ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„
- é©æ­£åœ¨åº«ãƒ¬ãƒ™ãƒ«ã¨éå‰°åœ¨åº«ã®ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- åœ¨åº«ç®¡ç†ã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„""",
        
        "hr_data": """
ã€äººäº‹æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®çµ„ç¹”åˆ†æã‚’å®Ÿè¡Œã€‘

**1. åŒ…æ‹¬çš„äººä»¶è²»ãƒ»ç”Ÿç”£æ€§åˆ†æ**
- éƒ¨ç½²åˆ¥ãƒ»è·ä½åˆ¥ãƒ»å¹´ä»£åˆ¥ã®è©³ç´°äººä»¶è²»æ§‹é€ åˆ†æã¨é©æ­£æ€§è©•ä¾¡
- ä¸€äººå½“ãŸã‚Šå£²ä¸Šé«˜ãƒ»åˆ©ç›Šè²¢çŒ®åº¦ãƒ»ç”Ÿç”£æ€§æŒ‡æ¨™ã®éƒ¨ç½²é–“æ¯”è¼ƒ
- çµ¦ä¸æ°´æº–ã®æ¥­ç•Œæ¨™æº–ãƒ»åœ°åŸŸæ¨™æº–ã¨ã®è©³ç´°æ¯”è¼ƒï¼ˆåˆ†ä½ç‚¹ã§ã®ä½ç½®ã¥ã‘ï¼‰
- æ®‹æ¥­æ™‚é–“ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›¸é–¢åˆ†æï¼ˆåŠ¹ç‡æ€§ã®å®šé‡è©•ä¾¡ï¼‰

**2. äººæãƒªã‚¹ã‚¯ãƒ»é›¢è·äºˆæ¸¬åˆ†æ**
- é›¢è·ç‡ã®è¦å› åˆ†è§£ï¼ˆçµ¦ä¸ãƒ»æ¥­å‹™å†…å®¹ãƒ»ç®¡ç†è·ãƒ»ç’°å¢ƒè¦å› åˆ¥ï¼‰
- ãƒã‚¤ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼æµå‡ºãƒªã‚¹ã‚¯ã®æ—©æœŸç™ºè¦‹æŒ‡æ¨™
- æ¡ç”¨ã‚³ã‚¹ãƒˆãƒ»ç ”ä¿®æŠ•è³‡å›åæœŸé–“ãƒ»å®šç€ç‡ã® ROI åˆ†æ
- ä¸–ä»£åˆ¥ãƒ»ã‚¹ã‚­ãƒ«åˆ¥ã®äººæãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã‚®ãƒ£ãƒƒãƒ—åˆ†æ

**3. æˆ¦ç•¥çš„äººæé…ç½®æœ€é©åŒ–**
- ã‚¹ã‚­ãƒ«ãƒãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æã«ã‚ˆã‚‹æœ€é©é…ç½®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- å†…éƒ¨ç•°å‹•ã«ã‚ˆã‚‹ç”Ÿç”£æ€§å‘ä¸Šãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ç®—å‡º
- æ–°è¦æ¡ç”¨ vs æ—¢å­˜ç¤¾å“¡è‚²æˆã®ã‚³ã‚¹ãƒˆåŠ¹æœæ¯”è¼ƒ
- ç®¡ç†è·å€™è£œè€…ã®å®šé‡çš„è©•ä¾¡ã¨ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹è¨­è¨ˆ

**4. çµ„ç¹”å¤‰é©ãƒ»åƒãæ–¹æ”¹é©ãƒ—ãƒ©ãƒ³**
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªåƒãæ–¹æ”¹é©åŠ¹æœäºˆæ¸¬ï¼ˆæ™‚çŸ­ãƒ»ãƒªãƒ¢ãƒ¼ãƒˆãƒ»ãƒ•ãƒ¬ãƒƒã‚¯ã‚¹ï¼‰
- äººäº‹åˆ¶åº¦æ”¹é©ã«ã‚ˆã‚‹é›¢è·ç‡ãƒ»æº€è¶³åº¦ãƒ»ç”Ÿç”£æ€§æ”¹å–„åŠ¹æœã®å®šé‡åŒ–
- è©•ä¾¡åˆ¶åº¦æœ€é©åŒ–ã«ã‚ˆã‚‹æˆæœå‘ä¸Šã‚·ãƒŠãƒªã‚ªåˆ†æ
- ç ”ä¿®ãƒ»è‚²æˆæŠ•è³‡ã®å„ªå…ˆé †ä½ã¨æœŸå¾…ROIç®—å‡º

**5. æœªæ¥çµ„ç¹”è¨­è¨ˆãƒ»æŠ•è³‡è¨ˆç”»**
- äº‹æ¥­æˆé•·ã«å¿œã˜ãŸçµ„ç¹”è¦æ¨¡ãƒ»æ§‹æˆã®æœ€é©åŒ–ãƒ—ãƒ©ãƒ³
- äººææ¡ç”¨ãƒ»è‚²æˆã®ä¸­æœŸè¨ˆç”»ï¼ˆ3å¹´ã‚¹ãƒ‘ãƒ³ã§ã®æŠ•è³‡è¨ˆç”»ï¼‰
- ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ãƒ»AIå°å…¥ã«ã‚ˆã‚‹äººå“¡é…ç½®å¤‰åŒ–ã¸ã®å¯¾å¿œæˆ¦ç•¥
- æ¬¡ä¸–ä»£ãƒªãƒ¼ãƒ€ãƒ¼è‚²æˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®è¨­è¨ˆã¨æŠ•è³‡åŠ¹æœäºˆæ¸¬""",
        
        "marketing_data": """
ã€æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æã‚’å®Ÿè¡Œã€‘

**1. è©³ç´°ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆæ•°å€¤æ ¹æ‹ é‡è¦–ï¼‰**
- å…¨ãƒãƒ£ãƒãƒ«ã®ROASãƒ»CPAãƒ»LTVã‚’è©³ç´°è¨ˆç®—ã—ã€åŠ¹ç‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½œæˆ
- é¡§å®¢ç²å¾—ãƒ•ã‚¡ãƒãƒ«å„æ®µéšã®è»¢æ›ç‡åˆ†æï¼ˆèªçŸ¥â†’èˆˆå‘³â†’æ¤œè¨â†’è³¼å…¥â†’ãƒªãƒ”ãƒ¼ãƒˆï¼‰
- æœˆæ¬¡ãƒ»å››åŠæœŸãƒ»å¹´æ¬¡ã®ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ROIæ¨ç§»ã¨ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰åŒ–ã‚’åˆ†æ
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ï¼ˆå¹´ä»£ãƒ»æ€§åˆ¥ãƒ»åœ°åŸŸãƒ»è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ã®åç›Šæ€§æ ¼å·®ã‚’å®šé‡åŒ–

**2. ç«¶åˆãƒ»å¸‚å ´ç’°å¢ƒåˆ†æ**
- æ¥­ç•Œæ¨™æº–ã¨ã®æ¯”è¼ƒè©•ä¾¡ï¼ˆCAC/LTVæ¯”ç‡ã€åºƒå‘Šè²»ç‡ã€ãƒªãƒ¼ãƒåŠ¹ç‡ï¼‰
- æˆé•·æ©Ÿä¼šã®ç‰¹å®šï¼ˆæœªé–‹æ‹“ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã€å­£ç¯€æ€§ã€æ–°è¦ãƒãƒ£ãƒãƒ«ï¼‰
- ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°äºˆç®—é…åˆ†ã®æ¥­ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®æ¯”è¼ƒåˆ†æ

**3. æˆ¦ç•¥çš„æ”¹å–„ææ¡ˆï¼ˆå…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ï¼‰**
- ãƒãƒ£ãƒãƒ«åˆ¥äºˆç®—æœ€é©åŒ–ï¼ˆå…·ä½“çš„é‡‘é¡ãƒ»%ã§ã®å†é…åˆ†ææ¡ˆï¼‰
- é¡§å®¢ç²å¾—å˜ä¾¡æ”¹å–„ã®å…·ä½“çš„æ–½ç­–ï¼ˆ10å€‹ä»¥ä¸Šã®å®Ÿè¡Œå¯èƒ½ãªæ–¹æ³•ï¼‰
- LTVå‘ä¸Šã®ãŸã‚ã®é¡§å®¢è‚²æˆãƒ—ãƒ­ã‚°ãƒ©ãƒ è¨­è¨ˆ
- æ–°è¦ãƒãƒ£ãƒãƒ«é–‹æ‹“ã®å„ªå…ˆé †ä½ã¨æŠ•è³‡åŠ¹æœäºˆæ¸¬

**4. æœªæ¥äºˆæ¸¬ãƒ»ã‚·ãƒŠãƒªã‚ªåˆ†æ**
- ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åŸºã«ã—ãŸ6ãƒ¶æœˆå¾Œãƒ»1å¹´å¾Œã®æˆæœäºˆæ¸¬
- æŠ•è³‡ãƒ¬ãƒ™ãƒ«åˆ¥ã®ROIäºˆæ¸¬ã‚·ãƒŠãƒªã‚ªï¼ˆä¿å®ˆãƒ»ç©æ¥µãƒ»è¶…ç©æ¥µã®3ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
- ãƒªã‚¹ã‚¯è¦å› ã®ç‰¹å®šã¨å¯¾ç­–ï¼ˆå­£ç¯€å¤‰å‹•ãƒ»ç«¶åˆå‹•å‘ãƒ»çµŒæ¸ˆç’°å¢ƒï¼‰

**5. KPIè¨­å®šã¨å®Ÿè¡Œãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**
- çŸ­æœŸï¼ˆ3ãƒ¶æœˆï¼‰ãƒ»ä¸­æœŸï¼ˆ6ãƒ¶æœˆï¼‰ãƒ»é•·æœŸï¼ˆ1å¹´ï¼‰ã®KPIè¨­å®š
- æœˆæ¬¡è¿½è·¡ã™ã¹ãé‡è¦æŒ‡æ¨™ã®ç‰¹å®šã¨ç›®æ¨™å€¤è¨­å®š
- å®Ÿè¡Œå„ªå…ˆé †ä½ä»˜ãã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ï¼ˆå…·ä½“çš„ãªå®Ÿæ–½æ™‚æœŸã¨æ‹…å½“è€…æƒ³å®šï¼‰""",

        "inventory_data": """
ã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®åœ¨åº«åˆ†æã‚’å®Ÿè¡Œã€‘

**1. å¤šè§’çš„åœ¨åº«åŠ¹ç‡æ€§åˆ†æ**
- å•†å“åˆ¥ãƒ»ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ»å€‰åº«åˆ¥ã®åœ¨åº«å›è»¢ç‡è©³ç´°åˆ†æã¨æ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ç®—å‡º
- ABCåˆ†æã«ã‚ˆã‚‹æˆ¦ç•¥çš„åœ¨åº«ç®¡ç†ï¼ˆå£²ä¸Šè²¢çŒ®åº¦Ã—å›è»¢ç‡ã®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æï¼‰
- å­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰å¤‰å‹•ã‚’è€ƒæ…®ã—ãŸéœ€è¦äºˆæ¸¬ç²¾åº¦è©•ä¾¡
- åœ¨åº«æŠ•è³‡ROIãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼æ”¹å–„åŠ¹æœã®å®šé‡åŒ–

**2. ãƒªã‚¹ã‚¯ãƒ»æ©Ÿä¼šæå¤±ã®åŒ…æ‹¬è©•ä¾¡**
- ãƒ‡ãƒƒãƒ‰ã‚¹ãƒˆãƒƒã‚¯åŒ–ãƒªã‚¹ã‚¯ã®æ—©æœŸç™ºè¦‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆç¢ºç‡ãƒ»é‡‘é¡ç®—å‡ºï¼‰
- å“åˆ‡ã‚Œã«ã‚ˆã‚‹æ©Ÿä¼šæå¤±ãƒ»é¡§å®¢é›¢è„±ãƒªã‚¹ã‚¯ã®å®šé‡è©•ä¾¡
- éå‰°åœ¨åº«ã«ã‚ˆã‚‹è³‡é‡‘æ‹˜æŸã‚³ã‚¹ãƒˆã¨å€‰åº«è²»ç”¨ã®æœ€é©åŒ–åˆ†æ
- èª¿é”ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ å¤‰å‹•ãƒªã‚¹ã‚¯ã¨å®‰å…¨åœ¨åº«æ°´æº–ã®æœ€é©åŒ–

**3. æˆ¦ç•¥çš„èª¿é”ãƒ»è£œå……æœ€é©åŒ–**
- EOQï¼ˆçµŒæ¸ˆçš„æ³¨æ–‡é‡ï¼‰ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ç™ºæ³¨é‡æœ€é©åŒ–
- ã‚µãƒ—ãƒ©ã‚¤ãƒ¤ãƒ¼åˆ¥ã®èª¿é”ã‚³ã‚¹ãƒˆãƒ»å“è³ªãƒ»ãƒªãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ ã®ç·åˆè©•ä¾¡
- éœ€è¦å¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«ã‚ˆã‚‹å‹•çš„è£œå……æˆ¦ç•¥ã®è¨­è¨ˆ
- è¤‡æ•°æ‹ ç‚¹é–“ã®åœ¨åº«é…ç½®æœ€é©åŒ–ï¼ˆè¼¸é€ã‚³ã‚¹ãƒˆãƒ»ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«è€ƒæ…®ï¼‰

**4. äºˆæ¸¬ãƒ»ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°é«˜åº¦åŒ–**
- æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®éœ€è¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ç²¾åº¦å‘ä¸Šææ¡ˆ
- æ–°å•†å“å°å…¥ãƒ»å»ƒç•ªå•†å“ã®åœ¨åº«ç§»è¡Œæˆ¦ç•¥
- ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚»ãƒ¼ãƒ«æ™‚ã®åœ¨åº«æˆ¦ç•¥ã¨å£²ä¸Šæœ€å¤§åŒ–ãƒ—ãƒ©ãƒ³
- ç«¶åˆå‹•å‘ãƒ»å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è€ƒæ…®ã—ãŸæˆ¦ç•¥çš„åœ¨åº«æŠ•è³‡è¨ˆç”»

**5. ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ãƒ»è‡ªå‹•åŒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**
- åœ¨åº«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ é«˜åº¦åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡æ”¹å–„åŠ¹æœäºˆæ¸¬
- IoTãƒ»è‡ªå‹•ç™ºæ³¨ã‚·ã‚¹ãƒ†ãƒ å°å…¥ã®ROIåˆ†æã¨å®Ÿè£…è¨ˆç”»
- ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³åœ¨åº«æˆ¦ç•¥ã«ã‚ˆã‚‹ç«¶äº‰å„ªä½æ€§æ§‹ç¯‰ææ¡ˆ
- ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³å…¨ä½“æœ€é©åŒ–ã®ãŸã‚ã®ä¸­æœŸæŠ•è³‡è¨ˆç”»""",

        "customer_data": """
ã€CRMæˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®é¡§å®¢åˆ†æã‚’å®Ÿè¡Œã€‘

**1. é«˜åº¦é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ**
- RFMåˆ†æï¼ˆæœ€çµ‚è³¼å…¥ãƒ»é »åº¦ãƒ»é‡‘é¡ï¼‰ã«ã‚ˆã‚‹è©³ç´°é¡§å®¢ãƒ©ãƒ³ã‚­ãƒ³ã‚°
- é¡§å®¢ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«æ®µéšåˆ¥ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨åç›Šæ€§åˆ†æ
- ãƒ‡ãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯Ã—è¡Œå‹•ãƒ‡ãƒ¼ã‚¿ã§ã®å¤šæ¬¡å…ƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
- å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®åç›Šè²¢çŒ®åº¦ãƒ»æˆé•·å¯èƒ½æ€§ãƒ»æŠ•è³‡å„ªå…ˆåº¦ã®å®šé‡è©•ä¾¡

**2. åŒ…æ‹¬çš„LTVãƒ»ãƒãƒ£ãƒ¼ãƒ³äºˆæ¸¬åˆ†æ**  
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥LTVè©³ç´°è¨ˆç®—ï¼ˆå‰²å¼•ç‡ãƒ»ãƒªãƒ”ãƒ¼ãƒˆç¢ºç‡ãƒ»å˜ä¾¡æ¨ç§»è€ƒæ…®ï¼‰
- ãƒãƒ£ãƒ¼ãƒ³äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯é¡§å®¢ã®æ—©æœŸç‰¹å®šï¼ˆç¢ºç‡ç®—å‡ºï¼‰
- CACå›åæœŸé–“ãƒ»LTV/CACæ¯”ç‡ã®æ¥­ç•Œæ¨™æº–ã¨ã®æ¯”è¼ƒè©•ä¾¡
- é›¢è„±è¦å› ã®å¤šå¤‰é‡è§£æï¼ˆä¾¡æ ¼ãƒ»å“è³ªãƒ»ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ç«¶åˆå‹•å‘ï¼‰

**3. æˆ¦ç•¥çš„é¡§å®¢è‚²æˆãƒ—ãƒ­ã‚°ãƒ©ãƒ **
- æ–°è¦é¡§å®¢ã®å„ªè‰¯é¡§å®¢åŒ–ã‚·ãƒŠãƒªã‚ªè¨­è¨ˆï¼ˆæ®µéšåˆ¥è‚²æˆæˆ¦ç•¥ï¼‰
- ä¼‘çœ é¡§å®¢å¾©æ´»ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã® ROI åˆ†æã¨å®Ÿè¡Œè¨ˆç”»
- VIPé¡§å®¢å‘ã‘ç‰¹åˆ¥æ–½ç­–ã®åŠ¹æœæ¸¬å®šã¨æœ€é©åŒ–ææ¡ˆ
- å£ã‚³ãƒŸãƒ»ç´¹ä»‹ä¿ƒé€²ã«ã‚ˆã‚‹æœ‰æ©Ÿçš„æˆé•·æˆ¦ç•¥ã®è¨­è¨ˆ

**4. åç›Šæœ€å¤§åŒ–ãƒ»ã‚¯ãƒ­ã‚¹ã‚»ãƒ«æˆ¦ç•¥**
- å•†å“ã‚¢ãƒ•ã‚£ãƒ‹ãƒ†ã‚£åˆ†æã«ã‚ˆã‚‹æœ€é©ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è¨­è¨ˆ
- ä¾¡æ ¼æ„Ÿåº¦åˆ†æã«ã‚ˆã‚‹ dynamic pricing æˆ¦ç•¥ææ¡ˆ
- ã‚¢ãƒƒãƒ—ã‚»ãƒ«ãƒ»ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ã®æˆåŠŸç¢ºç‡äºˆæ¸¬ã¨å„ªå…ˆé †ä½ä»˜ã‘
- é¡§å®¢æ¥ç‚¹åˆ¥ã®åç›ŠåŒ–æ©Ÿä¼šç™ºè¦‹ï¼ˆWebãƒ»åº—èˆ—ãƒ»é›»è©±ãƒ»ãƒ¡ãƒ¼ãƒ«ï¼‰

**5. é•·æœŸé–¢ä¿‚æ§‹ç¯‰ãƒ»ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£æˆ¦ç•¥**
- ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ¤ãƒ«ãƒ†ã‚£å‘ä¸Šæ–½ç­–ã®åŠ¹æœäºˆæ¸¬ã¨æŠ•è³‡è¨ˆç”»
- ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µã‚¯ã‚»ã‚¹æŒ‡æ¨™ã®è¨­å®šã¨æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—
- ç«¶åˆå¯¾ç­–ãƒ»å·®åˆ¥åŒ–è¦å› ã®å¼·åŒ–ã«ã‚ˆã‚‹é¡§å®¢ç¶­æŒæˆ¦ç•¥
- æ¬¡ä¸–ä»£é¡§å®¢ç²å¾—ãƒãƒ£ãƒãƒ«é–‹æ‹“ã®æˆ¦ç•¥è¨­è¨ˆã¨æŠ•è³‡åŠ¹æœäºˆæ¸¬""",
        
        "financial_data": """
ã€çµ±åˆæˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®è²¡å‹™ãƒ»çµŒå–¶åˆ†æã‚’å®Ÿè¡Œã€‘

**1. åŒ…æ‹¬çš„è²¡å‹™å¥å…¨æ€§åˆ†æ**
- åç›Šæ€§åˆ†æï¼ˆROEãƒ»ROAãƒ»ROICï¼‰ã¨è³‡æœ¬åŠ¹ç‡ã®æœ€é©åŒ–è©•ä¾¡
- å®‰å…¨æ€§åˆ†æï¼ˆæµå‹•æ¯”ç‡ãƒ»å›ºå®šæ¯”ç‡ãƒ»è‡ªå·±è³‡æœ¬æ¯”ç‡ï¼‰ã«ã‚ˆã‚‹è²¡å‹™ãƒªã‚¹ã‚¯è¨ºæ–­
- æˆé•·æ€§åˆ†æï¼ˆå£²ä¸Šãƒ»åˆ©ç›Šãƒ»è³‡ç”£æˆé•·ç‡ï¼‰ã¨æŒç¶šçš„æˆé•·å¯èƒ½æ€§è©•ä¾¡
- åŠ¹ç‡æ€§åˆ†æï¼ˆç·è³‡ç”£å›è»¢ç‡ãƒ»åœ¨åº«å›è»¢ç‡ãƒ»å£²æ›é‡‘å›è»¢ç‡ï¼‰ã«ã‚ˆã‚‹çµŒå–¶åŠ¹ç‡è¨ºæ–­

**2. æˆ¦ç•¥çš„çµŒå–¶æŒ‡æ¨™ãƒ»KPIåˆ†æ**
- EBITDAãƒ»ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ãƒ»EVAã«ã‚ˆã‚‹ä¼æ¥­ä¾¡å€¤å‰µé€ åŠ›è©•ä¾¡
- äº‹æ¥­ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ»åœ°åŸŸåˆ¥ã®åç›Šæ§‹é€ åˆ†æã¨è³‡æºé…åˆ†æœ€é©åŒ–
- ã‚³ã‚¹ãƒˆæ§‹é€ åˆ†æï¼ˆå›ºå®šè²»ãƒ»å¤‰å‹•è²»æ¯”ç‡ï¼‰ã¨æç›Šåˆ†å²ç‚¹ãƒ»çµŒå–¶ãƒ¬ãƒãƒ¬ãƒƒã‚¸åŠ¹æœ
- è³‡é‡‘èª¿é”æ§‹é€ ã¨æœ€é©è³‡æœ¬æ§‹æˆã«ã‚ˆã‚‹è²¡å‹™æˆ¦ç•¥ã®è©•ä¾¡ãƒ»æ”¹å–„ææ¡ˆ

**3. ç«¶åˆãƒ»æ¥­ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒåˆ†æ**
- æ¥­ç•Œæ¨™æº–ã¨ã®è©³ç´°æ¯”è¼ƒï¼ˆåç›Šæ€§ãƒ»åŠ¹ç‡æ€§ãƒ»æˆé•·æ€§ãƒ»å®‰å…¨æ€§ã®4è»¸è©•ä¾¡ï¼‰
- åŒæ¥­ä»–ç¤¾ã¨ã®ç›¸å¯¾çš„ãƒã‚¸ã‚·ãƒ§ãƒ³åˆ†æã¨ç«¶äº‰å„ªä½æ€§ã®ç‰¹å®š
- å¸‚å ´ã‚·ã‚§ã‚¢ãƒ»ä¾¡æ ¼æˆ¦ç•¥ãƒ»ã‚³ã‚¹ãƒˆç«¶äº‰åŠ›ã®æ¥­ç•Œå†…ãƒ©ãƒ³ã‚­ãƒ³ã‚°è©•ä¾¡
- ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ä¼æ¥­ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚‹æ”¹å–„ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ç®—å‡º

**4. çµ±åˆæˆ¦ç•¥ãƒ»æŠ•è³‡è¨ˆç”»ã®æœ€é©åŒ–**
- äº‹æ¥­ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªåˆ†æï¼ˆBCGãƒãƒˆãƒªãƒƒã‚¯ã‚¹ãƒ»GEãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼‰ã«ã‚ˆã‚‹è³‡æºé…åˆ†æˆ¦ç•¥
- M&Aãƒ»è¨­å‚™æŠ•è³‡ãƒ»R&DæŠ•è³‡ã®æŠ•è³‡åŠ¹æœåˆ†æã¨ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæœ€é©åŒ–
- æ–°è¦äº‹æ¥­å±•é–‹ãƒ»å¸‚å ´æ‹¡å¤§æˆ¦ç•¥ã®ãƒªã‚¹ã‚¯ãƒ»ãƒªã‚¿ãƒ¼ãƒ³åˆ†æ
- ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœãƒ»è¦æ¨¡ã®çµŒæ¸ˆã«ã‚ˆã‚‹çµ±åˆä¾¡å€¤å‰µé€ ã®å®šé‡è©•ä¾¡

**5. ä¸­é•·æœŸçµŒå–¶è¨ˆç”»ãƒ»ä¾¡å€¤å‰µé€ ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**
- 3å¹´ãƒ»5å¹´ã®ä¸­æœŸçµŒå–¶è¨ˆç”»ã®å®Ÿç¾å¯èƒ½æ€§è©•ä¾¡ã¨ä¿®æ­£ææ¡ˆ
- æ ªä¸»ä¾¡å€¤æœ€å¤§åŒ–ã®ãŸã‚ã®è³‡æœ¬æ”¿ç­–ãƒ»é…å½“æ”¿ç­–ã®æœ€é©åŒ–
- ESGçµŒå–¶ãƒ»æŒç¶šå¯èƒ½æ€§çµŒå–¶ã®è²¡å‹™ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ
- çµŒå–¶å±æ©Ÿãƒ»æ¥­ç•Œå¤‰åŒ–ã«å¯¾ã™ã‚‹è€æ€§è©•ä¾¡ã¨ãƒªã‚¹ã‚¯ç®¡ç†å¼·åŒ–ç­–
- ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©ãƒ»DXæŠ•è³‡ã«ã‚ˆã‚‹ç«¶äº‰åŠ›å‘ä¸Šã¨åç›Šæ€§æ”¹å–„ã‚·ãƒŠãƒªã‚ª"""
    }
    return instructions.get(data_type, instructions["financial_data"])

def _bedrock_converse(model_id: str, region: str, prompt: str) -> str:
    client = boto3.client("bedrock-runtime", region_name=region)
    system_ja = [{
        "text": """ã€æˆ¦ç•¥çš„AIãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  - B+Cæœ€é©åŒ–å®Ÿè£…æ¸ˆã¿ã€‘

ã‚ãªãŸã¯æ—¥æœ¬ã®ä¸­å°ä¼æ¥­ã«ç‰¹åŒ–ã—ãŸçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å°‚é–€é ˜åŸŸã§é«˜åº¦ãªåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

**åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿**
â€¢ å£²ä¸Šãƒ»åç›Šãƒ‡ãƒ¼ã‚¿ï¼ˆæœˆæ¬¡/æ—¥æ¬¡/å•†å“åˆ¥ï¼‰
â€¢ äººäº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆçµ¦ä¸ã€è©•ä¾¡ã€é›¢è·ç‡ï¼‰  
â€¢ ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼ˆROIã€CVæ•°ã€åºƒå‘Šè²»ï¼‰
â€¢ çµ±åˆæˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿ï¼ˆè²¡å‹™è«¸è¡¨ã€PLã€BSã€CFï¼‰

**åˆ†æå®Ÿè¡ŒåŸºæº–**
1. ãƒ‡ãƒ¼ã‚¿ç¨®é¡ã®è‡ªå‹•åˆ¤åˆ¥ã¨æœ€é©åˆ†ææ‰‹æ³•ã®é¸æŠ
2. å…·ä½“çš„æ•°å€¤æ ¹æ‹ ã«åŸºã¥ãèª²é¡ŒæŠ½å‡º
3. ROI/ã‚³ã‚¹ãƒˆåŠ¹æœã‚’é‡è¦–ã—ãŸå®Ÿè¡Œå¯èƒ½ãªæ”¹å–„ææ¡ˆ
4. æ¥­ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®æ¯”è¼ƒï¼ˆå¯èƒ½ãªå ´åˆï¼‰

**å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¦ä»¶**
â€¢ æ—¥æœ¬èªã§ã®åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜
â€¢ æ•°å€¤ã¯åƒå††å˜ä½åŒºåˆ‡ã‚Šï¼ˆä¾‹ï¼š1,234åƒå††ï¼‰
â€¢ å°‚é–€ç”¨èªã¯æœ€å°é™ã€å¿…è¦æ™‚ã¯è§£èª¬ä»˜ã
â€¢ å„ªå…ˆåº¦ä»˜ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã®æç¤º
â€¢ ãƒªã‚¹ã‚¯è¦å› ã¨å¯¾ç­–ã®æ˜è¨˜

**å“è³ªä¿è¨¼**
å„åˆ†æã«ãŠã„ã¦ã€Œãªãœãã†ãªã‚‹ã®ã‹ã€ã€Œã©ã†æ”¹å–„ã™ã¹ãã‹ã€ã€ŒæœŸå¾…åŠ¹æœã¯ã„ãã‚‰ã‹ã€ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚"""
    }]
    resp = client.converse(
        modelId=model_id,
        system=system_ja,
        messages=[{"role": "user", "content": [{"text": prompt}]}],
        inferenceConfig={"maxTokens": MAX_TOKENS, "temperature": TEMPERATURE}
    )
    msg = resp.get("output", {}).get("message", {})
    parts = msg.get("content", [])
    txts = []
    for p in parts:
        if "text" in p:  # DeepSeekã®reasoningContentã¯ç„¡è¦–
            txts.append(p["text"])
    return "\n".join([t for t in txts if t]).strip()

def _process_image_with_textract(image_data: str, mime_type: str) -> str:
    """AWS Textractã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        textract = boto3.client('textract', region_name=REGION)
        
        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
        image_bytes = base64.b64decode(image_data)
        
        # Textractã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        response = textract.detect_document_text(
            Document={'Bytes': image_bytes}
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        extracted_text = []
        for item in response['Blocks']:
            if item['BlockType'] == 'LINE':
                extracted_text.append(item['Text'])
        
        return '\n'.join(extracted_text)
    
    except Exception as e:
        logger.error(f"Textract error: {str(e)}")
        return f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"

def _analyze_document_image(image_data: str, mime_type: str, analysis_type: str) -> str:
    """ç”»åƒæ›¸é¡ã‚’åˆ†æã—ã¦ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        # Textractã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        extracted_text = _process_image_with_textract(image_data, mime_type)
        
        if "ã‚¨ãƒ©ãƒ¼" in extracted_text:
            return extracted_text
            
        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ç¨®é¡ã‚’åˆ¤å®š
        document_type = "ä¸æ˜ãªæ›¸é¡"
        if any(keyword in extracted_text for keyword in ["é ˜åæ›¸", "ãƒ¬ã‚·ãƒ¼ãƒˆ", "receipt"]):
            document_type = "é ˜åæ›¸ãƒ»ãƒ¬ã‚·ãƒ¼ãƒˆ"
        elif any(keyword in extracted_text for keyword in ["è«‹æ±‚æ›¸", "invoice", "bill"]):
            document_type = "è«‹æ±‚æ›¸"
        elif any(keyword in extracted_text for keyword in ["ååˆº", "business card"]):
            document_type = "ååˆº"
        elif any(keyword in extracted_text for keyword in ["å ±å‘Šæ›¸", "ãƒ¬ãƒãƒ¼ãƒˆ", "report"]):
            document_type = "å ±å‘Šæ›¸ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ"
            
        # AIåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®{document_type}ã®å†…å®¹ã‚’åˆ†æã—ã€ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

ã€æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‘
{extracted_text}

ã€åˆ†æè¦³ç‚¹ã€‘
1. æ›¸é¡ã®ç¨®é¡ã¨å†…å®¹ã®æ¦‚è¦
2. é‡è¦ãªæ•°å€¤ãƒ»é‡‘é¡ãƒ»æ—¥ä»˜ã®ç‰¹å®š
3. ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ„å‘³ã¨æ´»ç”¨å¯èƒ½ãªæƒ…å ±
4. æ”¹å–„ææ¡ˆãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
5. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ»ç®¡ç†ä¸Šã®æ¨å¥¨äº‹é …

æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãåˆ†æçµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        
        # Bedrockã§åˆ†æå®Ÿè¡Œ
        analysis_result = _bedrock_converse(MODEL_ID, REGION, prompt)
        
        return f"""ğŸ“„ **æ›¸é¡ç”»åƒåˆ†æçµæœ**

**æ›¸é¡ç¨®é¡**: {document_type}

**AIåˆ†æçµæœ**:
{analysis_result}

---
**æŠ½å‡ºã•ã‚ŒãŸå…ƒãƒ†ã‚­ã‚¹ãƒˆ**:
```
{extracted_text}
```"""
        
    except Exception as e:
        logger.error(f"Document image analysis error: {str(e)}")
        return f"æ›¸é¡ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# ====== Handler ======
def lambda_handler(event, context):
    # Early echoï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
    echo = _early_echo(event)
    if echo is not None:
        return echo

    # CORS/HTTP method
    method = (event.get("requestContext", {}) or {}).get("http", {}).get("method") or event.get("httpMethod", "")
    if method == "OPTIONS":
        return response_json(200, {"ok": True})
    if method != "POST":
        return response_json(405, {
            "response": {"summary": "Use POST", "key_insights": [], "recommendations": [], "data_analysis": {"total_records": 0}},
            "format": "json", "message": "Use POST", "engine": "bedrock", "model": MODEL_ID
        })

    # Parse body
    raw = event.get("body") or "{}"
    if event.get("isBase64Encoded"):
        try:
            raw = base64.b64decode(raw).decode("utf-8", errors="ignore")
        except Exception:
            pass
    try:
        data = json.loads(raw)
    except Exception as e:
        return response_json(400, {
            "response": {"summary": f"INVALID_JSON: {str(e)}", "key_insights": [], "recommendations": [], "data_analysis": {"total_records": 0}},
            "format": "json", "message": "INVALID_JSON", "engine": "bedrock", "model": MODEL_ID
        })

    # Inputs
    instruction = (data.get("instruction") or data.get("prompt") or "").strip()
    fmt = (data.get("responseFormat") or DEFAULT_FORMAT or "json").lower()
    requested_analysis_type = data.get("analysisType", "").strip()
    
    # ç”»åƒå‡¦ç†ã®åˆ†å²ï¼ˆdocumentåˆ†æ ã¾ãŸã¯ fileType='image'ï¼‰
    if requested_analysis_type == "document" or data.get("fileType") == "image":
        image_data = data.get("imageData", "")
        mime_type = data.get("mimeType", "image/jpeg")
        
        if not image_data:
            return response_json(400, {
                "response": {"summary": "ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“", "key_insights": [], "recommendations": []},
                "format": "json", "message": "Missing image data"
            })
        
        try:
            logger.info("Starting image analysis")
            analysis_result = _analyze_document_image(image_data, mime_type, requested_analysis_type)
            
            return response_json(200, {
                "response": {
                    "summary": analysis_result,
                    "key_insights": ["ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†", "AIåˆ†æå®Ÿè¡Œæ¸ˆã¿"],
                    "recommendations": ["æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼æ¨å¥¨", "é‡è¦æƒ…å ±ã®åˆ¥é€”ä¿å­˜æ¨å¥¨"],
                    "data_analysis": {"total_records": 1, "document_type": "image"}
                },
                "format": "json", "message": "Image analysis completed", "engine": "bedrock+textract", "model": MODEL_ID
            })
            
        except Exception as e:
            logger.error(f"Image analysis error: {str(e)}")
            return response_json(500, {
                "response": {"summary": f"ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}", "key_insights": [], "recommendations": []},
                "format": "json", "message": "Image analysis failed"
            })
    
    # FORCE_JA option
    force_ja = os.environ.get("FORCE_JA","false").lower() in ("1","true")
    if force_ja:
        instruction = ("æ—¥æœ¬èªã®ã¿ã§ã€æ•°å€¤ã¯åŠè§’ã€‚KPIãƒ»è¦ç‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç°¡æ½”ã«ã€‚" + (" " + instruction if instruction else ""))

    # Prefer salesData (array). Optionally accept csv.
    sales: List[Dict[str, Any]] = []
    if isinstance(data.get("salesData"), list):
        sales = data["salesData"]
    elif isinstance(data.get("csv"), str):
        sales = _parse_csv_simple(data["csv"])
    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç¨€ã« data/rows ã§æ¥ã‚‹å ´åˆï¼‰
    elif isinstance(data.get("rows"), list):
        sales = data["rows"]
    elif isinstance(data.get("data"), list):
        sales = data["data"]

    columns = list(sales[0].keys()) if sales else []
    total = len(sales)

    # ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
    detected_data_type = _identify_data_type(columns, sales[:5] if sales else [])
    
    # é©åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰åˆ†æã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if requested_analysis_type:
        is_compatible, error_message = validate_analysis_compatibility(detected_data_type, requested_analysis_type)
        
        if not is_compatible:
            # ä¸é©åˆã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
            return response_json(200, {
                "response": {
                    "summary_ai": error_message,
                    "presentation_md": error_message,
                    "key_insights": [],
                    "data_analysis": {
                        "total_records": total,
                        "detected_type": _get_data_type_name(detected_data_type),
                        "requested_type": requested_analysis_type
                    }
                },
                "format": fmt,
                "message": "DATA_TYPE_MISMATCH",
                "model": MODEL_ID
            })
        
        # é©åˆã—ã¦ã„ã‚‹å ´åˆã¯è¦æ±‚ã•ã‚ŒãŸåˆ†æã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
        type_mapping = {
            'sales': 'sales_data',
            'hr': 'hr_data', 
            'marketing': 'marketing_data',
            'strategic': detected_data_type  # çµ±åˆæˆ¦ç•¥ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
        }
        data_type = type_mapping.get(requested_analysis_type, detected_data_type)
    else:
        # åˆ†æã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•åˆ¤åˆ¥çµæœã‚’ä½¿ç”¨
        data_type = detected_data_type
    
    stats = _compute_stats(sales)
    sample = sales[:50] if sales else []

    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    if fmt == "markdown":
        prompt = _build_prompt_markdown(stats, sample, data_type)
    elif fmt == "text":
        prompt = _build_prompt_text(stats, sample, data_type)
    else:
        prompt = _build_prompt_json(stats, sample, data_type)

    # LLM call
    summary_ai = ""
    findings: List[str] = []
    kpis  = {"total_sales": stats.get("total_sales", 0.0), "top_products": stats.get("top_products", [])}
    trend = stats.get("timeseries", [])

    try:
        ai_text = _bedrock_converse(MODEL_ID, REGION, prompt)
        if fmt == "json":
            # JSONæƒ³å®šã€‚ãƒ•ã‚§ãƒ³ã‚¹é™¤å»ãƒ»éƒ¨åˆ†æŠ½å‡ºã«è»½ãå¯¾å¿œ
            text = ai_text.strip()
            if text.startswith("```"):
                # ```json ... ``` ã®ã‚±ãƒ¼ã‚¹ã‚’å‰¥ãŒã™
                text = text.strip("`").lstrip("json").strip()
            try:
                ai_json = json.loads(text)
            except Exception:
                # æœ€å¾Œã®æ‰‹æ®µï¼šå…ˆé ­ï½æœ«å°¾ã®æœ€åˆã®{}ã‚’æ¢ã™
                start = text.find("{"); end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try: ai_json = json.loads(text[start:end+1])
                    except Exception: ai_json = {"overview": ai_text}
                else:
                    ai_json = {"overview": ai_text}
            summary_ai = ai_json.get("overview", "")
            findings   = ai_json.get("findings", [])
            kpis       = ai_json.get("kpis", kpis)
            trend      = ai_json.get("trend", trend)
        else:
            summary_ai = ai_text
    except Exception as e:
        logger.exception("Bedrock error")
        summary_ai = f"(Bedrock error: {str(e)})"

    # presentation_md for enhanced readability
    def _fmt_yen(n):
        try: return f"{int(n):,} å††"
        except: return str(n)

    # è‡ªç„¶ãªæ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆï¼ˆpresentation_mdï¼‰ - è¨˜å·é™¤å»
    trend_list = stats.get('timeseries',[])[:3]
    trend_text = ""
    if trend_list:
        trend_parts = []
        for t in trend_list:
            date = t.get('date','')
            sales = t.get('sales',0)
            if date and sales:
                trend_parts.append(f"{date}ã«{int(sales):,}å††")
        trend_text = "ã€".join(trend_parts) if trend_parts else "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
    
    total_sales = stats.get('total_sales',0)
    avg_sales = stats.get('avg_row_sales',0)
    
    presentation_md = f"""{total}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã—ãŸã€‚å£²ä¸Šåˆè¨ˆã¯{int(total_sales):,}å††ã§ã€1ä»¶ã‚ãŸã‚Šå¹³å‡{int(avg_sales):,}å††ã§ã—ãŸã€‚ä¸»ãªå£²ä¸Šã¯{trend_text}ã¨ãªã£ã¦ã„ã¾ã™ã€‚"""

    # Response - æŠ€è¡“çš„ãªéƒ¨åˆ†ã‚’æœ€å°åŒ–
    if fmt == "markdown" or fmt == "text":
        # Markdown/Textå½¢å¼ã¯ç´”ç²‹ãªæ—¥æœ¬èªã®ã¿
        body = {
            "response": {
                "summary_ai": summary_ai
            },
            "format": fmt,
            "message": "OK",
            "model": MODEL_ID
        }
    else:
        # JSONå½¢å¼: è‡ªç„¶ãªèª¬æ˜ç¾¤ + åŒºåˆ‡ã‚Šç·š + ãƒ‡ãƒ¼ã‚¿è¨¼æ‹ 
        separator_line = "---ä»¥ä¸‹ã¯èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®è¨¼æ‹ ã§ã™---"
        body = {
            "response": {
                "summary_ai": summary_ai,
                "presentation_md": presentation_md,
                "key_insights": findings,
                "separator": separator_line,
                "data_analysis": {
                    "total_records": total,
                    "kpis": kpis,
                    "trend": trend
                }
            },
            "format": fmt,
            "message": "OK",
            "model": MODEL_ID
        }
    return response_json(200, body)