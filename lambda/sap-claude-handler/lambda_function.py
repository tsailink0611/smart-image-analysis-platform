# lambda_function.py
# Stable, no external deps. Reads salesData (array) or csv (string). Bedrock converse. CORS/OPTIONS ready.

import json, os, base64, logging, boto3
from collections import Counter, defaultdict
from typing import Any, Dict, List, Optional, Tuple

# ====== ENV ======
MODEL_ID       = os.environ.get("BEDROCK_MODEL_ID", "us.deepseek.r1-v1:0")
REGION         = os.environ.get("AWS_REGION", os.environ.get("AWS_DEFAULT_REGION", "us-east-1"))
DEFAULT_FORMAT = (os.environ.get("DEFAULT_FORMAT", "json") or "json").lower()  # 'json'|'markdown'|'text'
MAX_TOKENS     = int(os.environ.get("MAX_TOKENS", "2200"))
TEMPERATURE    = float(os.environ.get("TEMPERATURE", "0.15"))

# ====== LOG ======
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ====== CORS/Response ======
def response_json(status: int, body: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "statusCode": status,
        "headers": {
            "Content-Type": "application/json; charset=utf-8",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Content-Type, Authorization, X-Requested-With",
            "Access-Control-Allow-Methods": "OPTIONS,POST"
        },
        "body": json.dumps(body, ensure_ascii=False)
    }

# ====== Debug early echo (enable with LAMBDA_DEBUG_ECHO=1 or ?echo=1) ======
def _early_echo(event: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    try:
        qs = (event.get("rawQueryString") or "").lower()
        env_on = os.environ.get("LAMBDA_DEBUG_ECHO") in ("1", "true", "TRUE")
        if not (env_on or ("echo=1" in qs)):
            return None
        body_raw = event.get("body")
        if event.get("isBase64Encoded") and isinstance(body_raw, str):
            try:
                body_raw = base64.b64decode(body_raw).decode("utf-8-sig")
            except Exception:
                body_raw = "<base64 decode error>"
        elif isinstance(body_raw, (bytes, bytearray)):
            try:
                body_raw = body_raw.decode("utf-8-sig")
            except Exception:
                body_raw = body_raw.decode("utf-8", errors="ignore")
        sample = body_raw[:1000] if isinstance(body_raw, str) else str(type(body_raw))
        return response_json(200, {
            "message": "DEBUG",
            "format": "json",
            "engine": "bedrock",
            "model": MODEL_ID,
            "response": {
                "echo": "early",
                "received_type": type(body_raw).__name__ if body_raw is not None else "None",
                "raw_sample": sample
            }
        })
    except Exception:
        return None

# ====== Helpers ======
def _to_number(x: Any) -> float:
    try:
        s = str(x).replace(",", "").replace("Â¥", "").replace("å††", "").strip()
        return float(s)
    except Exception:
        return 0.0

def _detect_columns(rows: List[Dict[str, Any]]) -> Dict[str, str]:
    colmap: Dict[str, str] = {}
    if not rows:
        return colmap
    for c in rows[0].keys():
        name = str(c)
        lc = name.lower()
        if ("æ—¥" in name) or ("date" in lc):
            colmap.setdefault("date", name)
        if ("å£²" in name) or ("é‡‘é¡" in name) or ("amount" in lc) or ("sales" in lc) or ("total" in lc):
            colmap.setdefault("sales", name)
        if ("å•†" in name) or ("å“" in name) or ("product" in lc) or ("item" in lc) or ("name" in lc):
            colmap.setdefault("product", name)
    return colmap

def _compute_stats(rows: List[Dict[str, Any]]) -> Dict[str, Any]:
    total = len(rows)
    if total == 0:
        return {"total_rows": 0, "total_sales": 0.0, "avg_row_sales": 0.0, "top_products": [], "timeseries": []}

    colmap = _detect_columns(rows)
    dcol, scol, pcol = colmap.get("date"), colmap.get("sales"), colmap.get("product")

    ts = defaultdict(float)
    by_product: Counter = Counter()
    total_sales = 0.0

    for r in rows:
        v = _to_number(r.get(scol, 0)) if scol else 0.0
        total_sales += v
        if pcol:
            by_product[str(r.get(pcol, "")).strip()] += v
        if dcol:
            dt = str(r.get(dcol, "")).strip().replace("/", "-")
            day = dt[:10] if len(dt) >= 10 else dt
            if day:
                ts[day] += v

    top_products = [{"name": k, "sales": float(v)} for k, v in by_product.most_common(5)]
    trend = [{"date": d, "sales": float(v)} for d, v in sorted(ts.items())]
    avg = float(total_sales / total) if total else 0.0

    return {
        "total_rows": total,
        "total_sales": float(total_sales),
        "avg_row_sales": avg,
        "top_products": top_products,
        "timeseries": trend
    }

def _build_prompt_json(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    schema_hint = {
        "type": "object",
        "properties": {
            "overview": {"type": "string"},
            "findings": {"type": "array", "items": {"type": "string"}},
            "kpis": {
                "type": "object",
                "properties": {
                    "total_sales": {"type": "number"},
                    "top_products": {
                        "type": "array",
                        "items": {"type": "object", "properties": {"name": {"type": "string"}, "sales": {"type": "number"}}}
                    }
                }
            },
            "trend": {"type": "array", "items": {"type": "object", "properties": {"date": {"type": "string"}, "sales": {"type": "number"}}}}
        },
        "required": ["overview", "findings", "kpis"]
    }
    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†ææŒ‡ç¤º
    analysis_instructions = _get_analysis_instructions(data_type)
    
    return f"""ã‚ãªãŸã¯ä¼æ¥­ã®è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹çµŒé¨“è±Šå¯ŒãªçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€çµŒå–¶é™£ã«åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ‡ãƒ¼ã‚¿ç¨®åˆ¥ã€‘: {_get_data_type_name(data_type)}

ã€åˆ†ææŒ‡ç¤ºã€‘
{analysis_instructions}

ã€å‡ºåŠ›å½¢å¼ã€‘
- è‡ªç„¶ãªæ—¥æœ¬èªã§ã€ã¾ã‚‹ã§åŒåƒšã«èª¬æ˜ã™ã‚‹ã‚ˆã†ã«æ›¸ã„ã¦ãã ã•ã„
- å°‚é–€ç”¨èªã¯å¿…è¦æœ€å°é™ã«ç•™ã‚ã€èª°ã§ã‚‚ç†è§£ã§ãã‚‹è¡¨ç¾ã‚’ä½¿ã£ã¦ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹åƒå††ã€ãªã©ã€æ—¥æœ¬äººãŒæ™®æ®µä½¿ã†è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„
- çµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ãªå½¢ã§æ•´ç†ã—ã¦ãã ã•ã„ï¼š
  - ã€Œå…¨ä½“ã®çŠ¶æ³ã€: ãƒ‡ãƒ¼ã‚¿ã®æ¦‚è¦ã‚’2-3è¡Œã§
  - ã€Œæ°—ã¥ã„ãŸã“ã¨ã€: é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’3ã¤ã¾ã§
  - ã€Œæ•°å­—ã®ã¾ã¨ã‚ã€: ä¸»è¦ãªæŒ‡æ¨™

â€»ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ã£ã¦åˆ†æã—ã¦ãã ã•ã„ï¼ˆæ¨æ¸¬ã¯é¿ã‘ã¦ãã ã•ã„ï¼‰
â€»ä»¥ä¸‹ã®å½¢å¼ã§JSONã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„: {json.dumps(schema_hint, ensure_ascii=False)}

[çµ±è¨ˆè¦ç´„]
{json.dumps(stats, ensure_ascii=False)}

[ã‚µãƒ³ãƒ—ãƒ«è¡Œ]
{json.dumps(sample, ensure_ascii=False)}
"""

def _build_prompt_markdown(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    return f"""ã‚ãªãŸã¯ä¼šç¤¾ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€ç¤¾é•·ã‚„éƒ¨é•·ãŒèª­ã‚€ãƒ¬ãƒãƒ¼ãƒˆã‚’ã€å®Œå…¨ã«æ—¥æœ¬èªã¨æ•°å­—ã ã‘ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ã€‘
- Markdownã‚„è¨˜å·ã¯ä¸€åˆ‡ä½¿ã‚ãšã€æ™®é€šã®æ—¥æœ¬èªæ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„
- ã€Œ##ã€ã€Œ**ã€ã€Œ|ã€ã€Œ-ã€ãªã©ã®è¨˜å·ã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„
- è‹±èªã‚„å°‚é–€ç”¨èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„
- ã¾ã‚‹ã§éƒ¨ä¸‹ãŒä¸Šå¸ã«å£é ­ã§å ±å‘Šã™ã‚‹ã‚ˆã†ãªã€è‡ªç„¶ãªæ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹%å¢—åŠ ã€ãªã©ã€æ—¥æœ¬äººãŒè©±ã™ã¨ãã®è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„

# çµ±è¨ˆè¦ç´„
{json.dumps(stats, ensure_ascii=False)}

# ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ï¼‰
{json.dumps(sample, ensure_ascii=False)}
"""

def _build_prompt_text(stats: Dict[str, Any], sample: List[Dict[str, Any]], data_type: str = "sales_data") -> str:
    return f"""ã‚ãªãŸã¯ä¼šç¤¾ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹ãƒ“ã‚¸ãƒã‚¹ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ä»¥ä¸‹ã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¦ã€ä¸Šå¸ã«å£é ­ã§å ±å‘Šã™ã‚‹ã‚ˆã†ã«ã€å®Œå…¨ã«æ—¥æœ¬èªã ã‘ã§3è¡Œä»¥å†…ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã€çµ¶å¯¾å®ˆã‚‹ã“ã¨ã€‘
- è¨˜å·ã€è‹±èªã€ã‚«ã‚¿ã‚«ãƒŠå°‚é–€ç”¨èªã¯ä¸€åˆ‡ä½¿ã‚ãªã„ã§ãã ã•ã„
- æ•°å­—ã¯ã€Œâ—‹â—‹ä¸‡å††ã€ã€Œâ—‹â—‹%å¢—åŠ ã€ãªã©ã€æ™®é€šã«è©±ã™ã¨ãã®è¡¨ç¾ã§æ›¸ã„ã¦ãã ã•ã„
- ã¾ã‚‹ã§æœç¤¼ã§å ±å‘Šã™ã‚‹ã‚ˆã†ãªã€è‡ªç„¶ãªè©±ã—è¨€è‘‰ã§æ›¸ã„ã¦ãã ã•ã„
- ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ã§ã€ä¸å¯§ã«æ›¸ã„ã¦ãã ã•ã„

[çµ±è¨ˆè¦ç´„]
{json.dumps(stats, ensure_ascii=False)}

[ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€å¤§50ï¼‰]
{json.dumps(sample, ensure_ascii=False)}
"""

def _parse_csv_simple(csv_text: str) -> List[Dict[str, Any]]:
    lines = [l for l in csv_text.splitlines() if l.strip() != ""]
    if not lines: return []
    headers = [h.strip() for h in lines[0].split(",")]
    rows: List[Dict[str, Any]] = []
    for line in lines[1:]:
        cells = [c.strip() for c in line.split(",")]
        row = {}
        for i, h in enumerate(headers):
            row[h] = cells[i] if i < len(cells) else ""
        rows.append(row)
    return rows

def _identify_data_type(columns: List[str], sample_data: List[Dict[str, Any]]) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã®åˆ—åã¨ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’è‡ªå‹•åˆ¤åˆ¥ï¼ˆ7ã¤ã®åˆ†æã‚¿ã‚¤ãƒ—ã«ç‰¹åŒ–ï¼‰"""
    if not columns:
        return "financial_data"
    
    # åˆ—åã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã¦åˆ¤åˆ¥ã—ã‚„ã™ãã™ã‚‹
    col_lower = [col.lower() for col in columns]
    col_str = " ".join(col_lower) + " " + " ".join(columns)
    
    # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ 
    scores = {
        "hr_data": 0,
        "marketing_data": 0,
        "sales_data": 0,
        "financial_data": 0,
        "inventory_data": 0,
        "customer_data": 0
    }
    
    # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
    hr_strong_keywords = ["ç¤¾å“¡id", "employee", "æ°å", "éƒ¨ç½²", "çµ¦ä¸", "salary", "è³ä¸", "å¹´å", "è©•ä¾¡", "performance", "æ®‹æ¥­", "overtime", "æœ‰çµ¦", "é›¢è·", "æ˜‡é€²", "ã‚¹ã‚­ãƒ«", "ãƒãƒ¼ãƒ è²¢çŒ®", "äººäº‹"]
    for keyword in hr_strong_keywords:
        if keyword in col_str:
            scores["hr_data"] += 3
    
    # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    hr_medium_keywords = ["å‹¤æ€ ", "attendance", "ç ”ä¿®", "training", "ç›®æ¨™é”æˆ", "è·ä½", "å…¥ç¤¾", "å¹´é½¢"]
    for keyword in hr_medium_keywords:
        if keyword in col_str:
            scores["hr_data"] += 2
    
    # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    marketing_strong_keywords = ["ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³", "campaign", "roi", "ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³", "impression", "ã‚¯ãƒªãƒƒã‚¯", "click", "cvæ•°", "conversion", "é¡§å®¢ç²å¾—", "cac", "roas", "åºƒå‘Š", "åª’ä½“", "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ"]
    for keyword in marketing_strong_keywords:
        if keyword in col_str:
            scores["marketing_data"] += 3
    
    # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    marketing_medium_keywords = ["äºˆç®—", "budget", "æ”¯å‡º", "cost", "facebook", "google", "youtube", "instagram", "tiktok", "twitter"]
    for keyword in marketing_medium_keywords:
        if keyword in col_str:
            scores["marketing_data"] += 1
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sales_strong_keywords = ["å£²ä¸Š", "sales", "revenue", "å•†å“", "product", "é¡§å®¢", "customer", "é‡‘é¡", "amount", "å˜ä¾¡", "price", "æ•°é‡", "quantity"]
    for keyword in sales_strong_keywords:
        if keyword in col_str:
            scores["sales_data"] += 3
    
    # å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    sales_medium_keywords = ["æ—¥ä»˜", "date", "åº—èˆ—", "store", "åœ°åŸŸ", "region", "ã‚«ãƒ†ã‚´ãƒª", "category"]
    for keyword in sales_medium_keywords:
        if keyword in col_str:
            scores["sales_data"] += 1
    
    # çµ±åˆæˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿ï¼ˆè²¡å‹™ãƒ‡ãƒ¼ã‚¿ï¼‰ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    financial_strong_keywords = ["å£²ä¸Šé«˜", "revenue", "åˆ©ç›Š", "profit", "è³‡ç”£", "asset", "è² å‚µ", "liability", "ã‚­ãƒ£ãƒƒã‚·ãƒ¥", "cash", "æç›Š", "pl", "è²¸å€Ÿ", "bs"]
    for keyword in financial_strong_keywords:
        if keyword in col_str:
            scores["financial_data"] += 3
    
    # åœ¨åº«åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    inventory_strong_keywords = ["åœ¨åº«", "inventory", "stock", "åœ¨åº«æ•°", "ä¿æœ‰æ•°", "å€‰åº«", "warehouse", "å›è»¢ç‡", "turnover", "æ»ç•™", "å…¥åº«", "å‡ºåº«", "èª¿é”", "procurement"]
    for keyword in inventory_strong_keywords:
        if keyword in col_str:
            scores["inventory_data"] += 3
    
    # åœ¨åº«åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    inventory_medium_keywords = ["å•†å“ã‚³ãƒ¼ãƒ‰", "sku", "ãƒ­ãƒƒãƒˆ", "lot", "å“ç•ª", "å‹ç•ª", "ä»•å…¥", "supplier", "ç™ºæ³¨", "order", "ç´æœŸ", "delivery"]
    for keyword in inventory_medium_keywords:
        if keyword in col_str:
            scores["inventory_data"] += 1
    
    # é¡§å®¢åˆ†æãƒ‡ãƒ¼ã‚¿ã®å¼·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
    customer_strong_keywords = ["é¡§å®¢", "customer", "ä¼šå“¡", "member", "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "user", "ltv", "lifetime", "churn", "é›¢è„±", "ç¶™ç¶š", "retention", "æº€è¶³åº¦", "satisfaction"]
    for keyword in customer_strong_keywords:
        if keyword in col_str:
            scores["customer_data"] += 3
    
    # é¡§å®¢åˆ†æãƒ‡ãƒ¼ã‚¿ã®ä¸­ç¨‹åº¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    customer_medium_keywords = ["ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ", "segment", "å¹´é½¢", "age", "æ€§åˆ¥", "gender", "åœ°åŸŸ", "region", "è³¼å…¥å±¥æ­´", "purchase", "ã‚¢ã‚¯ã‚»ã‚¹", "access", "ã‚¯ãƒªãƒƒã‚¯", "click"]
    for keyword in customer_medium_keywords:
        if keyword in col_str:
            scores["customer_data"] += 1
    
    # ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‹ã‚‰ã‚‚åˆ¤å®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    if sample_data and len(sample_data) > 0:
        sample = sample_data[0]
        
        # äººäº‹ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´çš„ãªå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
        for key, value in sample.items():
            str_value = str(value).lower()
            
            # äººäº‹ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(dept in str_value for dept in ["å–¶æ¥­éƒ¨", "itéƒ¨", "äººäº‹éƒ¨", "è²¡å‹™éƒ¨", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°éƒ¨"]):
                scores["hr_data"] += 5
            if any(pos in str_value for pos in ["ä¸»ä»»", "ä¿‚é•·", "ä¸€èˆ¬", "éƒ¨é•·", "èª²é•·"]):
                scores["hr_data"] += 3
            if any(risk in str_value for risk in ["ä½", "ä¸­", "é«˜"]) and ("ãƒªã‚¹ã‚¯" in key or "risk" in key.lower()):
                scores["hr_data"] += 4
                
            # ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(media in str_value for media in ["googleåºƒå‘Š", "facebookåºƒå‘Š", "youtubeåºƒå‘Š", "instagramåºƒå‘Š", "lineåºƒå‘Š", "tiktokåºƒå‘Š"]):
                scores["marketing_data"] += 5
            if "%" in str_value and any(metric in key.lower() for metric in ["roi", "é”æˆç‡", "æº€è¶³åº¦"]):
                scores["marketing_data"] += 2
                
            # å£²ä¸Šç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ•°å€¤ãŒå¤§ããã€å•†å“åãŒã‚ã‚‹å ´åˆï¼‰
            if "å•†å“" in key or "product" in key.lower():
                scores["sales_data"] += 3
            if key.lower() in ["åº—èˆ—", "store"] and str_value:
                scores["sales_data"] += 4
                
            # åœ¨åº«ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³
            if any(unit in str_value for unit in ["å€‹", "æœ¬", "kg", "ç®±", "ã‚»ãƒƒãƒˆ", "å°"]):
                scores["inventory_data"] += 2
            if "warehouse" in key.lower() or "å€‰åº«" in key:
                scores["inventory_data"] += 3
            if any(status in str_value for status in ["å…¥è·å¾…ã¡", "å‡ºè·æ¸ˆã¿", "åœ¨åº«åˆ‡ã‚Œ", "èª¿é”ä¸­"]):
                scores["inventory_data"] += 4
                
            # é¡§å®¢ç³»ã®å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³  
            if any(age in str_value for age in ["20ä»£", "30ä»£", "40ä»£", "50ä»£", "60ä»£"]) or str_value.isdigit() and 18 <= int(str_value) <= 80:
                scores["customer_data"] += 3
            if any(gender in str_value for gender in ["ç”·æ€§", "å¥³æ€§", "male", "female", "ç”·", "å¥³"]):
                scores["customer_data"] += 3
            if "@" in str_value:  # ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹
                scores["customer_data"] += 4
    
    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¿ã‚¤ãƒ—ã‚’è¿”ã™
    if max(scores.values()) > 0:
        return max(scores, key=scores.get)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return "financial_data"

def _get_data_type_name(data_type: str) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®æ—¥æœ¬èªåã‚’è¿”ã™"""
    type_names = {
        "pl_statement": "æç›Šè¨ˆç®—æ›¸ï¼ˆPLè¡¨ï¼‰",
        "balance_sheet": "è²¸å€Ÿå¯¾ç…§è¡¨ï¼ˆBSï¼‰",
        "cashflow_statement": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸",
        "sales_data": "å£²ä¸Šãƒ‡ãƒ¼ã‚¿",
        "inventory_data": "åœ¨åº«ãƒ‡ãƒ¼ã‚¿",
        "customer_data": "é¡§å®¢ãƒ‡ãƒ¼ã‚¿",
        "hr_data": "äººäº‹ãƒ‡ãƒ¼ã‚¿",
        "marketing_data": "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿",
        "financial_data": "è²¡å‹™ãƒ‡ãƒ¼ã‚¿",
        "document_data": "æ›¸é¡ç”»åƒãƒ‡ãƒ¼ã‚¿",
        "unknown": "ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿"
    }
    return type_names.get(data_type, "è²¡å‹™ãƒ‡ãƒ¼ã‚¿")

def validate_analysis_compatibility(detected_data_type: str, requested_analysis_type: str) -> Tuple[bool, str]:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã¨åˆ†æã‚¿ã‚¤ãƒ—ã®é©åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆä½¿ã„ã‚„ã™ã•é‡è¦–ï¼‰"""
    # é©åˆæ€§ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ - ã‚ˆã‚ŠæŸ”è»Ÿã«
    compatibility_matrix = {
        'sales': {
            'primary': ['sales_data'],  # ä¸»è¦å¯¾å¿œ
            'secondary': ['financial_data'],  # å‰¯æ¬¡å¯¾å¿œï¼ˆè­¦å‘Šãªã—ã§é€šã™ï¼‰
            'name': 'å£²ä¸Šåˆ†æ',
            'description': 'å£²ä¸Šãƒ»å•†å“ãƒ»é¡§å®¢ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'hr': {
            'primary': ['hr_data'],
            'secondary': [],  # äººäº‹ã¯å³å¯†ã«
            'name': 'äººäº‹åˆ†æ', 
            'description': 'å¾“æ¥­å“¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»çµ¦ä¸ãƒ»è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'marketing': {
            'primary': ['marketing_data'],
            'secondary': ['financial_data'],  # äºˆç®—ãƒ‡ãƒ¼ã‚¿ãªã©ã‚‚å¯
            'name': 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°åˆ†æ',
            'description': 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ»ROIãƒ»é¡§å®¢ç²å¾—ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ'
        },
        'strategic': {
            'primary': ['financial_data', 'sales_data'],
            'secondary': ['hr_data', 'marketing_data'],  # çµ±åˆæˆ¦ç•¥ã¯ä½•ã§ã‚‚å¯
            'name': 'çµ±åˆæˆ¦ç•¥åˆ†æ',
            'description': 'ç·åˆçš„ãªãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿ã®æˆ¦ç•¥åˆ†æ'
        }
    }
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯é€šã™
    if requested_analysis_type not in compatibility_matrix:
        return True, ""
    
    config = compatibility_matrix[requested_analysis_type]
    
    # ä¸»è¦ã‚¿ã‚¤ãƒ—ã¾ãŸã¯å‰¯æ¬¡ã‚¿ã‚¤ãƒ—ã«é©åˆã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    all_allowed = config['primary'] + config['secondary']
    
    if detected_data_type in all_allowed:
        return True, ""  # é©åˆã—ã¦ã„ã‚‹
    
    # ä¸é©åˆã®å ´åˆã®ã¿ã‚¨ãƒ©ãƒ¼
    if detected_data_type not in all_allowed:
        # æœ€é©ãªãƒœã‚¿ãƒ³ã‚’ææ¡ˆ
        best_match = None
        for btn_type, btn_config in compatibility_matrix.items():
            if detected_data_type in (btn_config['primary'] + btn_config['secondary']):
                best_match = btn_config['name']
                break
        
        error_msg = f"""âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ä¸ä¸€è‡´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ

ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿: {_get_data_type_name(detected_data_type)}
é¸æŠã•ã‚ŒãŸåˆ†æ: {config['name']}

ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯{config['name']}ã«ã¯æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"""
        
        if best_match:
            error_msg += f"\n\nğŸ’¡ ã“ã®ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€Œ{best_match}ã€ãŒãŠã™ã™ã‚ã§ã™ã€‚\n\nãŸã ã—ã€ãã®ã¾ã¾åˆ†æã‚’ç¶šè¡Œã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚"
            # è­¦å‘Šã ã‘ã§ç¶šè¡Œã‚’è¨±å¯
            return True, ""
        else:
            error_msg += f"\n\nã€Œçµ±åˆæˆ¦ç•¥åˆ†æã€ãƒœã‚¿ãƒ³ã‚’ãŠè©¦ã—ãã ã•ã„ã€‚"
            return True, ""
    
    return True, ""

def _get_analysis_instructions(data_type: str) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†ææŒ‡ç¤ºã‚’è¿”ã™"""
    instructions = {
        "pl_statement": """
- å£²ä¸Šé«˜ã€å£²ä¸ŠåŸä¾¡ã€ç²—åˆ©ç‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- è²©ç®¡è²»ã®å†…è¨³ã¨å£²ä¸Šé«˜ã«å ã‚ã‚‹å‰²åˆã‚’åˆ†æã—ã¦ãã ã•ã„
- å–¶æ¥­åˆ©ç›Šã€çµŒå¸¸åˆ©ç›Šã€å½“æœŸç´”åˆ©ç›Šã®æ¨ç§»ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- åç›Šæ€§ã®å¥å…¨æ€§ã¨æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„""",
        
        "balance_sheet": """
- ç·è³‡ç”£ã€æµå‹•è³‡ç”£ã€å›ºå®šè³‡ç”£ã®æ§‹æˆã‚’ç¢ºèªã—ã¦ãã ã•ã„
- è² å‚µã¨ç´”è³‡ç”£ã®ãƒãƒ©ãƒ³ã‚¹ã‚’åˆ†æã—ã¦ãã ã•ã„
- æµå‹•æ¯”ç‡ã€è‡ªå·±è³‡æœ¬æ¯”ç‡ãªã©ã®å®‰å…¨æ€§æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„
- è²¡å‹™ã®å¥å…¨æ€§ã¨è³‡é‡‘ç¹°ã‚Šã«ã¤ã„ã¦è©•ä¾¡ã—ã¦ãã ã•ã„""",
        
        "cashflow_statement": """
- å–¶æ¥­ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã€æŠ•è³‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã€è²¡å‹™ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- ç¾é‡‘å‰µå‡ºèƒ½åŠ›ã¨è³‡é‡‘ã®ä½¿ã„é“ã‚’åˆ†æã—ã¦ãã ã•ã„
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã®å¥å…¨æ€§ã¨æŒç¶šå¯èƒ½æ€§ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- è³‡é‡‘ç¹°ã‚Šã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„""",
        
        "sales_data": """
- å£²ä¸Šã®åˆè¨ˆã€å¹³å‡ã€ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- å•†å“åˆ¥ãƒ»æ™‚æœŸåˆ¥ã®å£²ä¸Šæ§‹æˆã‚’åˆ†æã—ã¦ãã ã•ã„
- å£²ä¸Šã®æˆé•·æ€§ã¨å­£ç¯€æ€§ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„
- å–¶æ¥­æˆ¦ç•¥ã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„""",
        
        "inventory_data": """
- åœ¨åº«ã®ç·é¡ã€å•†å“åˆ¥æ§‹æˆã‚’ç¢ºèªã—ã¦ãã ã•ã„
- åœ¨åº«å›è»¢ç‡ã‚„æ»ç•™åœ¨åº«ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„
- é©æ­£åœ¨åº«ãƒ¬ãƒ™ãƒ«ã¨éå‰°åœ¨åº«ã®ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- åœ¨åº«ç®¡ç†ã®æ”¹å–„ç‚¹ãŒã‚ã‚Œã°ææ¡ˆã—ã¦ãã ã•ã„""",
        
        "hr_data": """
- éƒ¨ç½²åˆ¥ãƒ»è·ä½åˆ¥ã®äººä»¶è²»åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„
- çµ¦ä¸æ°´æº–ã®é©æ­£æ€§ã¨æ¥­ç•Œæ¨™æº–ã¨ã®æ¯”è¼ƒã‚’ç¢ºèªã—ã¦ãã ã•ã„
- æ®‹æ¥­æ™‚é–“ã¨ç”Ÿç”£æ€§ã®é–¢ä¿‚ã‚’åˆ†æã—ã¦ãã ã•ã„
- é›¢è·ç‡ã‚„æ¡ç”¨ã‚³ã‚¹ãƒˆã®å‚¾å‘ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„
- äººå“¡é…ç½®ã®æœ€é©åŒ–ã¨åƒãæ–¹æ”¹é©ã®ææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„""",
        
        "marketing_data": """
- ãƒãƒ£ãƒãƒ«åˆ¥ã®åºƒå‘Šè²»å¯¾åŠ¹æœï¼ˆROASï¼‰ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„
- é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆï¼ˆCACï¼‰ã¨ç”Ÿæ¶¯ä¾¡å€¤ï¼ˆLTVï¼‰ã‚’åˆ†æã—ã¦ãã ã•ã„
- ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã¨ã‚¯ãƒªãƒƒã‚¯ç‡ã®æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„
- æœ€ã‚‚åŠ¹ç‡çš„ãªãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ–½ç­–ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
- äºˆç®—é…åˆ†ã®æœ€é©åŒ–ã¨ROIå‘ä¸Šç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„""",

        "inventory_data": """
- åœ¨åº«ç·é¡ã¨å•†å“åˆ¥åœ¨åº«æ§‹æˆã‚’åˆ†æã—ã¦ãã ã•ã„
- åœ¨åº«å›è»¢ç‡ã€å›è»¢æ—¥æ•°ã‚’è¨ˆç®—ã—æ¥­ç•Œæ¨™æº–ã¨æ¯”è¼ƒã—ã¦ãã ã•ã„
- æ»ç•™åœ¨åº«ã€ãƒ‡ãƒƒãƒ‰ã‚¹ãƒˆãƒƒã‚¯ã®ãƒªã‚¹ã‚¯ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
- å“åˆ‡ã‚Œãƒ»æ¬ å“ã«ã‚ˆã‚‹æ©Ÿä¼šæå¤±ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- é©æ­£åœ¨åº«ãƒ¬ãƒ™ãƒ«ã®è¨­å®šã¨èª¿é”è¨ˆç”»ã®æœ€é©åŒ–ã‚’ææ¡ˆã—ã¦ãã ã•ã„
- å­£ç¯€æ€§ãƒ»éœ€è¦å¤‰å‹•ã‚’è€ƒæ…®ã—ãŸåœ¨åº«ç®¡ç†æ”¹å–„ç­–ã‚’æç¤ºã—ã¦ãã ã•ã„""",

        "customer_data": """
- é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã®ç‰¹æ€§ã¨è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¦ãã ã•ã„
- LTVï¼ˆé¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤ï¼‰ã¨ãƒãƒ£ãƒ¼ãƒ³ç‡ï¼ˆé›¢è„±ç‡ï¼‰ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„
- é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆï¼ˆCACï¼‰ã¨LTVã®æ¯”ç‡ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
- å„ªè‰¯é¡§å®¢ã®ç‰¹å¾´ã¨ç¶­æŒæˆ¦ç•¥ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
- é¡§å®¢æº€è¶³åº¦å‘ä¸Šã¨ãƒªãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ”¹å–„ç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„
- ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ãƒ»ã‚¢ãƒƒãƒ—ã‚»ãƒ«æ©Ÿä¼šã®ç™ºè¦‹ã¨æ´»ç”¨æ–¹æ³•ã‚’æç¤ºã—ã¦ãã ã•ã„""",
        
        "financial_data": """
- ãƒ‡ãƒ¼ã‚¿ã®ä¸»è¦ãªé …ç›®ã¨æ•°å€¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„
- é‡è¦ãªæŒ‡æ¨™ã‚„æ¯”ç‡ãŒã‚ã‚Œã°è¨ˆç®—ã—ã¦ãã ã•ã„
- å‚¾å‘ã‚„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚Œã°åˆ†æã—ã¦ãã ã•ã„
- ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ„å‘³ã¨æ”¹å–„ç‚¹ãŒã‚ã‚Œã°æŒ‡æ‘˜ã—ã¦ãã ã•ã„"""
    }
    return instructions.get(data_type, instructions["financial_data"])

def _bedrock_converse(model_id: str, region: str, prompt: str) -> str:
    client = boto3.client("bedrock-runtime", region_name=region)
    system_ja = [{
        "text": """ã€æˆ¦ç•¥çš„AIãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  - B+Cæœ€é©åŒ–å®Ÿè£…æ¸ˆã¿ã€‘

ã‚ãªãŸã¯æ—¥æœ¬ã®ä¸­å°ä¼æ¥­ã«ç‰¹åŒ–ã—ãŸçµŒå–¶ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å°‚é–€é ˜åŸŸã§é«˜åº¦ãªåˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

**åˆ†æå¯¾è±¡ãƒ‡ãƒ¼ã‚¿**
â€¢ å£²ä¸Šãƒ»åç›Šãƒ‡ãƒ¼ã‚¿ï¼ˆæœˆæ¬¡/æ—¥æ¬¡/å•†å“åˆ¥ï¼‰
â€¢ äººäº‹ãƒ‡ãƒ¼ã‚¿ï¼ˆçµ¦ä¸ã€è©•ä¾¡ã€é›¢è·ç‡ï¼‰  
â€¢ ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼ˆROIã€CVæ•°ã€åºƒå‘Šè²»ï¼‰
â€¢ çµ±åˆæˆ¦ç•¥ãƒ‡ãƒ¼ã‚¿ï¼ˆè²¡å‹™è«¸è¡¨ã€PLã€BSã€CFï¼‰

**åˆ†æå®Ÿè¡ŒåŸºæº–**
1. ãƒ‡ãƒ¼ã‚¿ç¨®é¡ã®è‡ªå‹•åˆ¤åˆ¥ã¨æœ€é©åˆ†ææ‰‹æ³•ã®é¸æŠ
2. å…·ä½“çš„æ•°å€¤æ ¹æ‹ ã«åŸºã¥ãèª²é¡ŒæŠ½å‡º
3. ROI/ã‚³ã‚¹ãƒˆåŠ¹æœã‚’é‡è¦–ã—ãŸå®Ÿè¡Œå¯èƒ½ãªæ”¹å–„ææ¡ˆ
4. æ¥­ç•Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®æ¯”è¼ƒï¼ˆå¯èƒ½ãªå ´åˆï¼‰

**å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¦ä»¶**
â€¢ æ—¥æœ¬èªã§ã®åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜
â€¢ æ•°å€¤ã¯åƒå††å˜ä½åŒºåˆ‡ã‚Šï¼ˆä¾‹ï¼š1,234åƒå††ï¼‰
â€¢ å°‚é–€ç”¨èªã¯æœ€å°é™ã€å¿…è¦æ™‚ã¯è§£èª¬ä»˜ã
â€¢ å„ªå…ˆåº¦ä»˜ãã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ã®æç¤º
â€¢ ãƒªã‚¹ã‚¯è¦å› ã¨å¯¾ç­–ã®æ˜è¨˜

**å“è³ªä¿è¨¼**
å„åˆ†æã«ãŠã„ã¦ã€Œãªãœãã†ãªã‚‹ã®ã‹ã€ã€Œã©ã†æ”¹å–„ã™ã¹ãã‹ã€ã€ŒæœŸå¾…åŠ¹æœã¯ã„ãã‚‰ã‹ã€ã‚’å¿…ãšå«ã‚ã¦ãã ã•ã„ã€‚"""
    }]
    resp = client.converse(
        modelId=model_id,
        system=system_ja,
        messages=[{"role": "user", "content": [{"text": prompt}]}],
        inferenceConfig={"maxTokens": MAX_TOKENS, "temperature": TEMPERATURE}
    )
    msg = resp.get("output", {}).get("message", {})
    parts = msg.get("content", [])
    txts = []
    for p in parts:
        if "text" in p:  # DeepSeekã®reasoningContentã¯ç„¡è¦–
            txts.append(p["text"])
    return "\n".join([t for t in txts if t]).strip()

def _process_image_with_textract(image_data: str, mime_type: str) -> str:
    """AWS Textractã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        textract = boto3.client('textract', region_name=REGION)
        
        # Base64ãƒ‡ã‚³ãƒ¼ãƒ‰
        image_bytes = base64.b64decode(image_data)
        
        # Textractã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        response = textract.detect_document_text(
            Document={'Bytes': image_bytes}
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        extracted_text = []
        for item in response['Blocks']:
            if item['BlockType'] == 'LINE':
                extracted_text.append(item['Text'])
        
        return '\n'.join(extracted_text)
    
    except Exception as e:
        logger.error(f"Textract error: {str(e)}")
        return f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}"

def _analyze_document_image(image_data: str, mime_type: str, analysis_type: str) -> str:
    """ç”»åƒæ›¸é¡ã‚’åˆ†æã—ã¦ãƒ“ã‚¸ãƒã‚¹åˆ†æã‚’å®Ÿè¡Œ"""
    try:
        # Textractã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        extracted_text = _process_image_with_textract(image_data, mime_type)
        
        if "ã‚¨ãƒ©ãƒ¼" in extracted_text:
            return extracted_text
            
        # æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ç¨®é¡ã‚’åˆ¤å®š
        document_type = "ä¸æ˜ãªæ›¸é¡"
        if any(keyword in extracted_text for keyword in ["é ˜åæ›¸", "ãƒ¬ã‚·ãƒ¼ãƒˆ", "receipt"]):
            document_type = "é ˜åæ›¸ãƒ»ãƒ¬ã‚·ãƒ¼ãƒˆ"
        elif any(keyword in extracted_text for keyword in ["è«‹æ±‚æ›¸", "invoice", "bill"]):
            document_type = "è«‹æ±‚æ›¸"
        elif any(keyword in extracted_text for keyword in ["ååˆº", "business card"]):
            document_type = "ååˆº"
        elif any(keyword in extracted_text for keyword in ["å ±å‘Šæ›¸", "ãƒ¬ãƒãƒ¼ãƒˆ", "report"]):
            document_type = "å ±å‘Šæ›¸ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ"
            
        # AIåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®{document_type}ã®å†…å®¹ã‚’åˆ†æã—ã€ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ´å¯Ÿã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

ã€æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‘
{extracted_text}

ã€åˆ†æè¦³ç‚¹ã€‘
1. æ›¸é¡ã®ç¨®é¡ã¨å†…å®¹ã®æ¦‚è¦
2. é‡è¦ãªæ•°å€¤ãƒ»é‡‘é¡ãƒ»æ—¥ä»˜ã®ç‰¹å®š
3. ãƒ“ã‚¸ãƒã‚¹ä¸Šã®æ„å‘³ã¨æ´»ç”¨å¯èƒ½ãªæƒ…å ±
4. æ”¹å–„ææ¡ˆãƒ»æ³¨æ„ç‚¹ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
5. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ»ç®¡ç†ä¸Šã®æ¨å¥¨äº‹é …

æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãåˆ†æçµæœã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        
        # Bedrockã§åˆ†æå®Ÿè¡Œ
        analysis_result = _bedrock_converse(MODEL_ID, REGION, prompt)
        
        return f"""ğŸ“„ **æ›¸é¡ç”»åƒåˆ†æçµæœ**

**æ›¸é¡ç¨®é¡**: {document_type}

**AIåˆ†æçµæœ**:
{analysis_result}

---
**æŠ½å‡ºã•ã‚ŒãŸå…ƒãƒ†ã‚­ã‚¹ãƒˆ**:
```
{extracted_text}
```"""
        
    except Exception as e:
        logger.error(f"Document image analysis error: {str(e)}")
        return f"æ›¸é¡ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"

# ====== Handler ======
def lambda_handler(event, context):
    # Early echoï¼ˆå¿…è¦æ™‚ã®ã¿ï¼‰
    echo = _early_echo(event)
    if echo is not None:
        return echo

    # CORS/HTTP method
    method = (event.get("requestContext", {}) or {}).get("http", {}).get("method") or event.get("httpMethod", "")
    if method == "OPTIONS":
        return response_json(200, {"ok": True})
    if method != "POST":
        return response_json(405, {
            "response": {"summary": "Use POST", "key_insights": [], "recommendations": [], "data_analysis": {"total_records": 0}},
            "format": "json", "message": "Use POST", "engine": "bedrock", "model": MODEL_ID
        })

    # Parse body
    raw = event.get("body") or "{}"
    if event.get("isBase64Encoded"):
        try:
            raw = base64.b64decode(raw).decode("utf-8", errors="ignore")
        except Exception:
            pass
    try:
        data = json.loads(raw)
    except Exception as e:
        return response_json(400, {
            "response": {"summary": f"INVALID_JSON: {str(e)}", "key_insights": [], "recommendations": [], "data_analysis": {"total_records": 0}},
            "format": "json", "message": "INVALID_JSON", "engine": "bedrock", "model": MODEL_ID
        })

    # Inputs
    instruction = (data.get("instruction") or data.get("prompt") or "").strip()
    fmt = (data.get("responseFormat") or DEFAULT_FORMAT or "json").lower()
    requested_analysis_type = data.get("analysisType", "").strip()
    
    # ç”»åƒå‡¦ç†ã®åˆ†å²ï¼ˆdocumentåˆ†æ ã¾ãŸã¯ fileType='image'ï¼‰
    if requested_analysis_type == "document" or data.get("fileType") == "image":
        image_data = data.get("imageData", "")
        mime_type = data.get("mimeType", "image/jpeg")
        
        if not image_data:
            return response_json(400, {
                "response": {"summary": "ç”»åƒãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“", "key_insights": [], "recommendations": []},
                "format": "json", "message": "Missing image data"
            })
        
        try:
            logger.info("Starting image analysis")
            analysis_result = _analyze_document_image(image_data, mime_type, requested_analysis_type)
            
            return response_json(200, {
                "response": {
                    "summary": analysis_result,
                    "key_insights": ["ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†", "AIåˆ†æå®Ÿè¡Œæ¸ˆã¿"],
                    "recommendations": ["æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼æ¨å¥¨", "é‡è¦æƒ…å ±ã®åˆ¥é€”ä¿å­˜æ¨å¥¨"],
                    "data_analysis": {"total_records": 1, "document_type": "image"}
                },
                "format": "json", "message": "Image analysis completed", "engine": "bedrock+textract", "model": MODEL_ID
            })
            
        except Exception as e:
            logger.error(f"Image analysis error: {str(e)}")
            return response_json(500, {
                "response": {"summary": f"ç”»åƒåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}", "key_insights": [], "recommendations": []},
                "format": "json", "message": "Image analysis failed"
            })
    
    # FORCE_JA option
    force_ja = os.environ.get("FORCE_JA","false").lower() in ("1","true")
    if force_ja:
        instruction = ("æ—¥æœ¬èªã®ã¿ã§ã€æ•°å€¤ã¯åŠè§’ã€‚KPIãƒ»è¦ç‚¹ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ç°¡æ½”ã«ã€‚" + (" " + instruction if instruction else ""))

    # Prefer salesData (array). Optionally accept csv.
    sales: List[Dict[str, Any]] = []
    if isinstance(data.get("salesData"), list):
        sales = data["salesData"]
    elif isinstance(data.get("csv"), str):
        sales = _parse_csv_simple(data["csv"])
    # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç¨€ã« data/rows ã§æ¥ã‚‹å ´åˆï¼‰
    elif isinstance(data.get("rows"), list):
        sales = data["rows"]
    elif isinstance(data.get("data"), list):
        sales = data["data"]

    columns = list(sales[0].keys()) if sales else []
    total = len(sales)

    # ã¾ãšãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
    detected_data_type = _identify_data_type(columns, sales[:5] if sales else [])
    
    # é©åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰åˆ†æã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
    if requested_analysis_type:
        is_compatible, error_message = validate_analysis_compatibility(detected_data_type, requested_analysis_type)
        
        if not is_compatible:
            # ä¸é©åˆã®å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™
            return response_json(200, {
                "response": {
                    "summary_ai": error_message,
                    "presentation_md": error_message,
                    "key_insights": [],
                    "data_analysis": {
                        "total_records": total,
                        "detected_type": _get_data_type_name(detected_data_type),
                        "requested_type": requested_analysis_type
                    }
                },
                "format": fmt,
                "message": "DATA_TYPE_MISMATCH",
                "model": MODEL_ID
            })
        
        # é©åˆã—ã¦ã„ã‚‹å ´åˆã¯è¦æ±‚ã•ã‚ŒãŸåˆ†æã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
        type_mapping = {
            'sales': 'sales_data',
            'hr': 'hr_data', 
            'marketing': 'marketing_data',
            'strategic': detected_data_type  # çµ±åˆæˆ¦ç•¥ã¯å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨
        }
        data_type = type_mapping.get(requested_analysis_type, detected_data_type)
    else:
        # åˆ†æã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•åˆ¤åˆ¥çµæœã‚’ä½¿ç”¨
        data_type = detected_data_type
    
    stats = _compute_stats(sales)
    sample = sales[:50] if sales else []

    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    if fmt == "markdown":
        prompt = _build_prompt_markdown(stats, sample, data_type)
    elif fmt == "text":
        prompt = _build_prompt_text(stats, sample, data_type)
    else:
        prompt = _build_prompt_json(stats, sample, data_type)

    # LLM call
    summary_ai = ""
    findings: List[str] = []
    kpis  = {"total_sales": stats.get("total_sales", 0.0), "top_products": stats.get("top_products", [])}
    trend = stats.get("timeseries", [])

    try:
        ai_text = _bedrock_converse(MODEL_ID, REGION, prompt)
        if fmt == "json":
            # JSONæƒ³å®šã€‚ãƒ•ã‚§ãƒ³ã‚¹é™¤å»ãƒ»éƒ¨åˆ†æŠ½å‡ºã«è»½ãå¯¾å¿œ
            text = ai_text.strip()
            if text.startswith("```"):
                # ```json ... ``` ã®ã‚±ãƒ¼ã‚¹ã‚’å‰¥ãŒã™
                text = text.strip("`").lstrip("json").strip()
            try:
                ai_json = json.loads(text)
            except Exception:
                # æœ€å¾Œã®æ‰‹æ®µï¼šå…ˆé ­ï½æœ«å°¾ã®æœ€åˆã®{}ã‚’æ¢ã™
                start = text.find("{"); end = text.rfind("}")
                if start != -1 and end != -1 and end > start:
                    try: ai_json = json.loads(text[start:end+1])
                    except Exception: ai_json = {"overview": ai_text}
                else:
                    ai_json = {"overview": ai_text}
            summary_ai = ai_json.get("overview", "")
            findings   = ai_json.get("findings", [])
            kpis       = ai_json.get("kpis", kpis)
            trend      = ai_json.get("trend", trend)
        else:
            summary_ai = ai_text
    except Exception as e:
        logger.exception("Bedrock error")
        summary_ai = f"(Bedrock error: {str(e)})"

    # presentation_md for enhanced readability
    def _fmt_yen(n):
        try: return f"{int(n):,} å††"
        except: return str(n)

    # è‡ªç„¶ãªæ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆï¼ˆpresentation_mdï¼‰ - è¨˜å·é™¤å»
    trend_list = stats.get('timeseries',[])[:3]
    trend_text = ""
    if trend_list:
        trend_parts = []
        for t in trend_list:
            date = t.get('date','')
            sales = t.get('sales',0)
            if date and sales:
                trend_parts.append(f"{date}ã«{int(sales):,}å††")
        trend_text = "ã€".join(trend_parts) if trend_parts else "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
    
    total_sales = stats.get('total_sales',0)
    avg_sales = stats.get('avg_row_sales',0)
    
    presentation_md = f"""{total}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã—ãŸã€‚å£²ä¸Šåˆè¨ˆã¯{int(total_sales):,}å††ã§ã€1ä»¶ã‚ãŸã‚Šå¹³å‡{int(avg_sales):,}å††ã§ã—ãŸã€‚ä¸»ãªå£²ä¸Šã¯{trend_text}ã¨ãªã£ã¦ã„ã¾ã™ã€‚"""

    # Response - æŠ€è¡“çš„ãªéƒ¨åˆ†ã‚’æœ€å°åŒ–
    if fmt == "markdown" or fmt == "text":
        # Markdown/Textå½¢å¼ã¯ç´”ç²‹ãªæ—¥æœ¬èªã®ã¿
        body = {
            "response": {
                "summary_ai": summary_ai
            },
            "format": fmt,
            "message": "OK",
            "model": MODEL_ID
        }
    else:
        # JSONå½¢å¼: è‡ªç„¶ãªèª¬æ˜ç¾¤ + åŒºåˆ‡ã‚Šç·š + ãƒ‡ãƒ¼ã‚¿è¨¼æ‹ 
        separator_line = "---ä»¥ä¸‹ã¯èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®è¨¼æ‹ ã§ã™---"
        body = {
            "response": {
                "summary_ai": summary_ai,
                "presentation_md": presentation_md,
                "key_insights": findings,
                "separator": separator_line,
                "data_analysis": {
                    "total_records": total,
                    "kpis": kpis,
                    "trend": trend
                }
            },
            "format": fmt,
            "message": "OK",
            "model": MODEL_ID
        }
    return response_json(200, body)